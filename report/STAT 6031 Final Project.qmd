---
title: "STAT 6031 Final Project"
author: "Adriana Gonzalez Sanchez & Jack McMahon"
format:
  html:
    fig-align: center
    code-fold: true
    results: hide
    toc: true
    css: custom.css
    embed-resources: true
editor: visual
---

```{r, message = FALSE, warning = FALSE}

library(data.table)
library(ggplot2)
library(dplyr)
library(jsonlite)
library(reshape2)
library(knitr)
library(kableExtra)
library(caret)
library(MASS)
library(glmnet)
library(boot)
library(scales)
```

# Section 1 - Data Description

The dataset for this project can be accessed via the following link: <https://insideairbnb.com/get-the-data/>, under section "New Orleans, Louisiana, United States" and under the file "listings.csv.gz."

It contains 7,118 observations and 75 covariates related to Airbnb listings in the New Orleans, Louisiana area. For a detailed description of each covariate, please refer to the following document: <https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit?gid=1322284596#gid=1322284596>.

![](New Orleans.jpg){fig-align="center" width="637"}

# Section 2 - Data Cleaning

Our analysis aims to predict Airbnb prices in the New Orleans area using various metrics, with "price" serving as the dependent variable. To ensure the quality and reliability of the data, we began by cleaning the dataset. This initial step involved several key tasks, including the removal of dollar signs from the price column and filtering out rows with missing price information.

Further data preparation involved parsing the amenities column, which was in JSON format, to identify potentially significant amenities that could influence price predictions. We also removed rows containing unrealistic price values or listings with zero availability, as these could skew the analysis.

```{r, message = FALSE, warning = FALSE}

raw_data <- fread("Listings_New_Orleans.csv")
bnb_data <- raw_data

### Remove listings with no price data
bnb_data <- bnb_data |> mutate(price = as.numeric(gsub("[$,]", "", as.character(price))))
bnb_data <- bnb_data[nchar(price)>1]  # (!!!) drops roughly 1000 records (!!!)

### Parse Amenities JSON

# Fix up quotes, so we can parse this JSON
bnb_data[,amenities:=gsub('""','"' , amenities, fixed=TRUE)]
bnb_data[,amenities_list:=lapply(bnb_data$amenities, function(x) {parse_json(x)})]

# Collapse list items into a table, for easier review of what these look like...
all_amenities <- unlist(bnb_data[, amenities_list])  |> data.table()
all_amenities <- all_amenities[,.N,by=V1][order(-N)]

# Potential dummy variables for "significant" amenities
bnb_data[amenities_list %like% "Long term stays allowed",has_long_term:=1]
bnb_data[amenities_list %like% "Backyard",has_backyard:=1]
bnb_data[amenities_list %like% "Patio",has_patio:=1]
bnb_data[amenities_list %like% "Pool",has_pool:=1]
bnb_data[amenities_list %like% "Stainless",has_stainless:=1]
bnb_data[amenities_list %like% "Lake",has_lake:=1]
bnb_data[amenities_list %like% "Private",has_privacy:=1]
bnb_data[amenities_list %like% "Window guards",has_window_guards:=1]
bnb_data[amenities_list %like% "Indoor fireplace",has_indoor_fireplace:=1]

cols <- names(bnb_data)[names(bnb_data) %like% "has" & !names(bnb_data) %in% c("host_has_profile_pic", "has_availability")]
bnb_data[ , (cols) := lapply(.SD, nafill, fill=0), .SDcols = cols]

### Remove "unrealized" listings - these have anomalous prices AND zero availability
# Or, not available within the next 365 days.....
# These are likely to have never had an actual stay and therefore their pricing is not relevant to our model
high_low <- bnb_data[,quantile(price, probs=c(.025, 0.975))]
bnb_data <- bnb_data[!(((price>high_low[2] | price<high_low[1]) & availability_30 == 0) | availability_365 == 0)]


```

Next, we proceeded to select the most relevant variables for our analysis. We removed columns containing URLs, IDs, names, descriptions, and overviews, as well as columns with neighborhood information, since we already had latitude and longitude data. Additionally, we eliminated columns with redundant information or those filled entirely with missing values (NAs).

After removing unnecessary columns, we ensured the data was in a workable format by standardizing missing values, coding them all as NA (as some were empty cells and others were "N/A"). We also converted the boolean columns from "True/False" to 1/0, where 1 corresponded to True and 0 to False. Furthermore, we removed percentage symbols from certain columns to convert them into numeric values.

To improve the utility of the data, we modified the "host_since" column to reflect the number of years the host had been active, making it easier to model. In addition to these adjustments, we incorporated spatial data by adding the locations of the top 10 destinations in the area, as listed by TripAdvisor, and created a new column to count the number of points of interest within a 1 km radius of each rental.

Below, you can observe two New Orleans maps, showing the spatial location of Airbnbs in the New Orlans area as well as the top 10 spots:

::: panel-tabset
## Airbnb Location

![](Listing Prices.jpeg){fig-align="center" width="532"}

## Top 10 Location

![](Top 10 Spots.jpeg){fig-align="center" width="529"}
:::

With these modifications completed, our \`\`final dataset'' contained 5921 observations and 45 covariates. These cleaning and selection steps ensured that the dataset was both accurate and relevant for building a predictive model for Airbnb pricing in the New Orleans region.

```{r, message = FALSE, warning = FALSE}

# Select useful variables

bnb_data <- bnb_data %>% 
  dplyr::select(-listing_url, -scrape_id, -last_scraped, -source, -name,
                -description,-picture_url, -host_id, -host_url, -host_name,
                -host_about, -host_thumbnail_url, -host_picture_url,
                -bathrooms_text, -minimum_nights,- maximum_nights,
                -minimum_minimum_nights, -maximum_minimum_nights,
                -minimum_maximum_nights, -maximum_maximum_nights,
                -calendar_updated, -has_availability, -calendar_last_scraped,
                -first_review, -license, -neighborhood_overview,
                -host_verifications, -neighbourhood,-neighbourhood_group_cleansed,
                -property_type, -amenities,-host_listings_count,
                -host_neighbourhood, -last_review)

# Replace "N/A" and empty with NA in the entire dataset
bnb_data <- bnb_data %>%
  mutate(across(where(is.character), ~ na_if(., "N/A"))) %>%
  mutate(across(where(is.character), ~ na_if(., "")))

## Convert T/F to 1/0 --> true = 1, false = 0
bnb_data <- bnb_data %>%
  mutate(across(c(host_is_superhost, host_has_profile_pic, 
                  host_identity_verified, instant_bookable),
                ~ if_else(as.character(.) == "t", 1,
                          if_else(as.character(.) == "f", 0, NA_real_))))

# Modify host_since column to host for x years, easier to model
bnb_data <- bnb_data %>%
  mutate(host_since_year = gsub("host since ", "", host_since),
         host_since_year = as.Date(host_since_year),
         host_years = ifelse(!is.na(host_since_year),  
                             as.numeric(format(Sys.Date(), "%Y")) - as.numeric(format(host_since_year, "%Y")),NA)) %>%
  dplyr::select(-host_since, -host_since_year)

# Code host_location column as 1 if New Orleans, 0 otherwise
bnb_data <- bnb_data %>%
  mutate(host_location = if_else(host_location == "New Orleans, LA", 1, 0))

# Remove % symbol and convert to numeric
bnb_data$host_response_rate <- as.numeric(gsub("%", "",
                                               bnb_data$host_response_rate))
bnb_data$host_response_rate <- as.numeric(bnb_data$host_response_rate)

bnb_data$host_acceptance_rate <- as.numeric(gsub("%", "",
                                                 bnb_data$host_acceptance_rate))
bnb_data$host_acceptance_rate <- as.numeric(bnb_data$host_acceptance_rate)

## Remove calculated fields - these are duplicative
cols <- names(bnb_data)[names(bnb_data) %like% "calculated"]
bnb_data[ , (cols) := NULL]

### Add top 10 destination points from TripAdvisor
# This is meant to model how good the location of the BNB is....
# Represented as a count - so "6" means that the listing is within 1 km of 6 different attractions
### Add top 10 destination points from TripAdvisor
dest_points <- data.table("Rank" = seq(1:10),
                          "Location" = c("WW2 Museum", "French Quarter", "Frenchman Street", "Garden District", "Jackson Square", "Preservation Hall", "Mardi Gras World", "St. Louis Cemetary", "New Orleans City Park", "St. Louis Cathedral"),
                          "Lat" = c(29.943208928864262, 29.959424483411674, 29.964318181205662, 29.928845784748955, 29.95756024289618, 29.958610251707462, 29.941144698995455, 29.959608239041174, 29.993454400417903, 29.95815187771942),
                          "Long" = c(-90.07057952646183, -90.06491814669677, -90.05773360204412, -90.08380077303872,-90.06294615548948, -90.06534875344381, -90.0621223634231, -90.07117833809907, -90.09813780389588, -90.06370009040566))


### Find everything within 1 km of each point
# Non equi-join (https://www.r-bloggers.com/2021/02/the-unequalled-joy-of-non-equi-joins/)

# Get locations within 1 km of key points (Latitude only)
near_id <- data.table()

for (i in 1:nrow(dest_points)) {
  setkey(bnb_data, latitude)
  setkey(dest_points, Lat)
  close_lat <- dest_points[i][bnb_data, roll = .01, nomatch = NULL]
  
  
  # Now filter to within 1km - longitude
  setkey(close_lat, longitude)
  setkey(dest_points, Long)
  close_lat_long <- dest_points[i][close_lat, roll = 0.01, nomatch = NULL][,.(id)]
  
  near_id <- rbind(near_id, close_lat_long)
  rm(close_lat)
  rm(close_lat_long)
}

### Count up how many points of interest are within 1 km of a given rental
near_id <- as.data.table(near_id)
near_id <- near_id[,near_top_10 := .N, by=id]

### Remove some other columns
bnb_data <- bnb_data %>%
  dplyr::select(-maximum_nights_avg_ntm, -amenities_list)

# Add this new derived variable back into the main dataset
bnb_data <- near_id[bnb_data,on="id"]
bnb_data <- bnb_data[is.na(near_top_10), near_top_10:=0]|> unique()

rm(near_id)
rm(high_low)
rm(i)
rm(all_amenities)
rm(cols)

bnb_data <- bnb_data %>%
  dplyr::select(-id)

final_data <- bnb_data
```

# Section 3 - Exploratory Data Analysis

Once we had a dataset we could work with, our next step was to explore more in depth both binary and non-binary covariates.

For each non-binary covariate, we examined key statistics, including the mean, median, standard deviation, minimum, maximum, and the number of missing values. The table below summarizes these findings:

```{r, message = FALSE}

### Sample size
n <- nrow(final_data)
p <- ncol(final_data)

### Report summary statistics
summary_stats <- function(data)
  {
    results <- list()
    # Loop through each column in the dataset
    for (col in names(data)) 
      {
        # Check if the column is numeric and not binary
        unique_values <- unique(na.omit(data[[col]]))
        if (is.numeric(data[[col]]) && length(unique_values) > 2)
          {
            # Calculate summary statistics
            summary_stats <- c(Mean = round(mean(data[[col]], na.rm = TRUE), 3),
                               Median = round(median(data[[col]], na.rm = TRUE), 3),
                               SD = round(sd(data[[col]], na.rm = TRUE), 3),
                               Min = round(min(data[[col]], na.rm = TRUE), 3),
                               Max = round(max(data[[col]], na.rm = TRUE), 3),
                               NAs = sum(is.na(data[[col]])))
      
            # Store the results in a list
            results[[col]] <- summary_stats
          } 
        else 
          {
            message(paste("Skipping binary or non-numeric column:", col))
          }
      }
  
    # Convert the list to a data frame for easier viewing
    results_df <- as.data.frame(do.call(rbind, results))
    return(results_df)
  }

nonbinary_summary <- summary_stats(final_data)

```

```{r}

# Create a kable table and center the text in the cells
kable(nonbinary_summary, align = "ccc",
      caption = "Summary Statistics for Numeric Variables",
      format = "html", escape = FALSE)
  

```

Additionally, we also looked at their distribution by creating boxplots and density plots:

```{r}

## Look at density plots and boxplots of non-binary variables

create_plots <- function(data) {
  # Loop through each column in the dataset
  for (col in names(data)) {
    # Check if the column is numeric and not binary (contains values other than 0 and 1)
    unique_values <- unique(na.omit(data[[col]]))
    if (is.numeric(data[[col]]) && length(unique_values) > 2) {
      # Create boxplot
      boxplot(data[[col]], main = paste("Boxplot"), xlab = col, col = "lightblue")
      
      # Create density plot
      plot(density(data[[col]], na.rm = TRUE), main = paste("Density Plot"), xlab = col, col = "lightgreen")
    } 
  }
}

```

::: panel-tabset
## 1

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_1 <- final_data %>%
  dplyr::select(near_top_10, host_response_rate, host_acceptance_rate,
         host_total_listings_count)

create_plots(selected_columns_1)
```

## 2

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_2 <- final_data %>%
  dplyr::select(latitude, longitude, accommodates, bathrooms)

create_plots(selected_columns_2)
```

## 3

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_3 <- final_data %>%
  dplyr::select(bedrooms, beds, price, minimum_nights_avg_ntm)

create_plots(selected_columns_3)
```

## 4

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_4 <- final_data %>%
  dplyr::select(availability_30, availability_60, availability_90,
         availability_365)

create_plots(selected_columns_4)
```

## 5

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_5 <- final_data %>%
  dplyr::select(number_of_reviews, number_of_reviews_ltm,
         number_of_reviews_l30d, review_scores_rating)

create_plots(selected_columns_5)
```

## 6

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_6 <- final_data %>%
  dplyr::select(review_scores_accuracy, review_scores_cleanliness,
         review_scores_checkin, review_scores_communication)

create_plots(selected_columns_6)
```

## 7

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_7 <- final_data %>%
  dplyr::select(review_scores_location, review_scores_value,
         reviews_per_month, host_years)

create_plots(selected_columns_7)
```
:::

From the summary statistics and the distribution above, we observe that most of our covariates are skewed and multimodal.

Additionally, taking a closer look at our dependent variable price, we might want to consider some sort of transformation.

Look at outliers?

Regarding the binary variables, we looked at the percentage of 1s and 0s:

```{r}

### Report % of binary variables
report_binary_percentages <- function(data) {
  # Initialize an empty list to store results
  results <- list()
  
  # Loop through each column in the dataset
  for (col in names(data)) {
    # Check if the column is binary (contains only 0s, 1s, or NA values)
    unique_values <- unique(na.omit(data[[col]]))
    if (all(unique_values %in% c(0, 1))) {
      # Calculate the percentages of 0s and 1s, ignoring NAs
      percent_1 <- mean(data[[col]] == 1, na.rm = TRUE) * 100
      percent_0 <- mean(data[[col]] == 0, na.rm = TRUE) * 100
      
      # Store the results in a list
      results[[col]] <- c(`% of 1s` = percent_1, `% of 0s` = percent_0)
    }
  }
  
  # Convert the list to a data frame for easier viewing
  results_df <- as.data.frame(do.call(rbind, results))
  return(results_df)
}

binary_summary <- report_binary_percentages(final_data)

```

```{r}

# Create a kable table and center the text in the cells
kable(binary_summary, align = "ccc",
      caption = "Summary Statistics for Binary Variables",
      format = "html")

```

From the table above, we observe that most hosts provide information about their location, include profile pictures, and have verified identities. However, the majority of rentals lack amenities such as patios, backyards, pools, stainless steel appliances, lakeside views, or indoor fireplaces, among other features.

Correlation and outliers?

```{r}

### Explore Price
# -------------------------------------------
# Inspect distribution of prices
# Very right-skewed
#ggplot(final_data,aes(price)) + geom_histogram() + labs(title = "Distribution of AirBNB Prices - New Orleans")

# Log transform isn't perfect either...
#ggplot(final_data,aes(log(price))) + geom_histogram() + labs(title = "Distribution of log(AirBNB Prices) - New Orleans")

### Correlation Heat Map Attempt
#set.seed(123)

#numeric_data <- final_data %>%
#  dplyr::select(-neighbourhood_cleansed, -room_type,
#                -host_response_time)

#numeric_data <- as_tibble(numeric_data)

#correlation_matrix <- cor(numeric_data)

#correlation_matrix_dt <- as.data.table(as.table(correlation_matrix))

# Melt the correlation matrix for ggplot2
#cor_melted <- melt(correlation_matrix_dt, na.rm = TRUE)

# Create the heatmap
#ggplot(cor_melted, aes(V1, V2, fill = value)) +
#  geom_tile(color = "white") +  # Add white borders to the squares
#  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
#                       midpoint = 0, limit = c(-1, 1), 
#                       name = "Correlation") +  # Gradient fill for correlation values
 # geom_text(aes(label = round(value, 2)), color = "black") +  # Add correlation values in squares
 # theme_minimal() +  # Use a minimal theme
#  labs(title = "Correlation Matrix Heatmap", 
#       x = "", 
#       y = "") +  # Remove axis labels
#  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Angle x-axis labels



```

# Section 4 - Modeling

After carefully examining the covariates, we proceeded to the modeling phase. The dataset was split into a training set (80%) and a testing set (20%) to facilitate model selection and prediction analysis.

```{r}

set.seed(111)

# Get rid of high dimensional dummy variables, just for convenience
final_data[,c("host_location", "neighbourhood_cleansed", "host_response_rate", "host_acceptance_rate", "latitude", "longitude"):=NULL]

# I would keep host location, response and acceptance rate, latitude and longitude

# Get rid of rows with NAs
final_data <- na.omit(final_data)


# Create Dummy Variables
inds <- unique(final_data$host_response_time)
final_data[, (inds) := lapply(inds, function(x) {ifelse(host_response_time == x, 1, 0)})]
final_data[,host_response_time:=NULL] ### Drop original column
final_data[,`a few days or more`:=NULL] ### drop one dummy column, to avoid perfect collinearity

inds <- unique(final_data$room_type)
final_data[, (inds) := lapply(inds, function(x) {ifelse(room_type == x, 1, 0)})]
final_data[,room_type:=NULL] ### Drop original column
final_data[,`Shared room`:=NULL] ### drop one dummy column, to avoid perfect collinearity

final_data[,host_owns_gt_5:=ifelse(host_total_listings_count > 5, 1, 0)]
final_data[,host_owns_2_5:=ifelse(host_total_listings_count < 5 & host_total_listings_count > 1, 1,0 )]
final_data[,host_total_listings_count:=NULL]

# Add interaction variable for # of reviews * review 
# we suspect that the number of high review scores might matter
final_data[,review_score_ct_interaction:=review_scores_rating * number_of_reviews]


### Keep a reference dataset
model_data <- copy(final_data)
```

```{r}

### Set up train/test split
tt_split <- createDataPartition(
  y = model_data$price,
  p = .80, ## The percentage of data in the training set
  list = FALSE
)

data_train <- model_data[ tt_split,]
data_test <- model_data[ -tt_split,]
```

We first fitted a simple linear regression model using log(price) as our dependent variable:

```{r}

# scale price by log, to make the right-skewed distribution look more normal
data_train[,log_price := log(price)]
data_train[,price:=NULL]

data_test[,log_price := log(price)]
data_test[,price:=NULL]
```

```{r}

# Fit model
simple_lm <- lm(log_price~.,data=data_train)

par(mfrow = c(2,2))
plot(simple_lm)
```

By examining the residuals, we identified opportunities for improvement. As a result, we applied various models chosen through stepwise selection, Lasso, and Ridge regression to evaluate whether any of them could enhance the in-sample fit.

::: panel-tabset
## Stepwise

```{r}

null_model <- lm(log_price ~ 1, data = data_train[complete.cases(data_train)])
full_model <- lm(log_price ~ ., data = data_train[complete.cases(data_train)])

step_model <- step(null_model, scope = list(lower = null_model, 
                                         upper = full_model),
                       k = log(nrow(data_train)), trace = F)

par(mfrow = c(2,2))
plot(step_model)

```

## Lasso

```{r}

lasso_cv <- cv.glmnet(data_train[, !"log_price", with=FALSE] |> as.matrix(),
                         data_train[,log_price] |> as.matrix(),
                         alpha=1,
                         #lambda=10^seq(-2,log10(exp(4)),length=101),
                         nfolds=10)
#coef(lm_lasso_cv, lm_lasso_cv$lambda.min)
par(mfrow = c(2,2))
#plot(lm_lasso_cv)

lasso_model <- glmnet(data_train[, !"log_price", with=FALSE] |> as.matrix(),
                      data_train[,log_price] |> as.matrix(),
                      alpha=1,
                      lambda = lasso_cv$lambda.min)

lasso_fitted <- predict(lasso_cv,
                        newx = data_train[, !"log_price", with = FALSE] |> as.matrix(), 
                       s = lasso_cv$lambda.min)

# Compute residuals
lasso_residuals <- data_train[, log_price] - lasso_fitted

# Compute standardized residuals
std_residuals_lasso <- lasso_residuals / sd(lasso_residuals)

# 1. Residuals vs Fitted
plot(lasso_fitted, lasso_residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

# 2. Normal Q-Q Plot
qqnorm(lasso_residuals, main = "Normal Q-Q")
qqline(lasso_residuals, col = "red", lty = 2)

# 3. Scale-Location Plot
plot(lasso_fitted, sqrt(abs(std_residuals_lasso)),
     main = "Scale-Location",
     xlab = "Fitted Values",
     ylab = expression(sqrt("|Standardized residuals|")),
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

# 4. Residuals vs Leverage
# Leverage and Cook's distance are not directly available from glmnet.
# Approximate by treating leverage as the influence of fitted values.

# Approximation: Simulate leverage-like values (optional)
lasso_leverages <- hatvalues(lm(data_train[, log_price] ~ as.matrix(data_train[, !"log_price", with = FALSE])))

plot(lasso_leverages, lasso_residuals, 
     main = "Residuals vs Leverage",
     xlab = "Leverage",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

```

## Ridge

```{r}

ridge_cv <- cv.glmnet(data_train[, !"log_price", with=FALSE] |> as.matrix(),
                         data_train[,log_price] |> as.matrix(),
                         alpha=0,
                         #lambda=10^seq(-2,log10(exp(4)),length=101),
                         nfolds=10)

#coef(lm_ridge_cv, s=lm_ridge_cv$lambda.min)
par(mfrow = c(2,2))
#plot(lm_ridge_cv)

ridge_model <- glmnet(data_train[, !"log_price", with=FALSE] |> as.matrix(),
                      data_train[,log_price] |> as.matrix(),
                      alpha=0,
                      lambda = ridge_cv$lambda.min)

ridge_fitted <- predict(ridge_cv,
                        newx = data_train[, !"log_price", with = FALSE] |> as.matrix(), 
                       s = ridge_cv$lambda.min)

# Compute residuals
ridge_residuals <- data_train[, log_price] - ridge_fitted

# Compute standardized residuals
std_residuals_ridge <- ridge_residuals / sd(ridge_residuals)


# 1. Residuals vs Fitted
plot(ridge_fitted, ridge_residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

# 2. Normal Q-Q Plot
qqnorm(ridge_residuals, main = "Normal Q-Q")
qqline(ridge_residuals, col = "red", lty = 2)

# 3. Scale-Location Plot
plot(ridge_fitted, sqrt(abs(std_residuals_ridge)),
     main = "Scale-Location",
     xlab = "Fitted Values",
     ylab = expression(sqrt("|Standardized residuals|")),
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

# 4. Residuals vs Leverage
# Leverage and Cook's distance are not directly available from glmnet.
# Approximate by treating leverage as the influence of fitted values.

# Approximation: Simulate leverage-like values (optional)
ridge_leverages <- hatvalues(lm(data_train[, log_price] ~ as.matrix(data_train[, !"log_price", with = FALSE])))

plot(ridge_leverages, ridge_residuals, 
     main = "Residuals vs Leverage",
     xlab = "Leverage",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)
```
:::

Once we had examined their model diagnostics plots in sample, we further examined some in-sample performance metrics reported below:

```{r}

# In-sample average sum squared error (ASE)

ASE_lm <- mean(simple_lm$residuals^2)
MSE_lm <- summary(simple_lm)$sigma^2

ASE_step <- mean(step_model$residuals^2)
MSE_step <- summary(step_model)$sigma^2

lasso_residuals <- data_train[, log_price] - lasso_fitted
ASE_lasso <- mean(lasso_residuals^2)

non_zero_coefs <- sum(coef(lasso_model, s = lasso_cv$lambda.min) != 0)
df_lasso <- nrow(data_train) - non_zero_coefs  
RSS_lasso <- sum(lasso_residuals^2)
MSE_lasso <- RSS_lasso / df_lasso

ridge_residuals <- data_train[, log_price] - ridge_fitted
ASE_ridge <- mean(ridge_residuals^2)

#df_rdige <- nrow(data_train) - non_zero_coefs  
#RSS_lasso <- sum(residuals^2)
MSE_ridge <- 0

results_table <- tibble(
  Model = c("OLS", "Stepwise Regression", "Lasso", "Ridge"),
  ASE = c(ASE_lm, ASE_step, ASE_lasso, ASE_ridge),
  MSE = c(MSE_lm, MSE_step, MSE_lasso, MSE_ridge)
)

# Display the table using kable
kable(results_table, align = "cccc", caption = "Model Comparison: ASE and MSE",
      format = "html")
```

Then, we looked at their out-of-sample performance:

```{r}

x_test <- data_test %>%
  dplyr::select(-log_price)

y_hat_test_ols <- predict(simple_lm, x_test)
MSPE_lm <- mean((data_test$log_price - y_hat_test_ols)^2)

y_hat_test_step <- predict(step_model, x_test)
MSPE_step <- mean((data_test$log_price - y_hat_test_step)^2)

x_test <- as.matrix(x_test)
y_hat_test_lasso <- predict(lasso_model, x_test,
                            s = lasso_cv$lambda.min)
MSPE_lasso <- mean((data_test$log_price - y_hat_test_lasso)^2)

y_hat_test_ridge <- predict(ridge_model, x_test, s = ridge_cv$lambda.min)
MSPE_ridge <- mean((data_test$log_price - y_hat_test_ridge)^2)

results_table <- tibble(
  Model = c("OLS", "Stepwise Regression", "Lasso", "Ridge"),
  MSPE = c(MSPE_lm, MSPE_step, MSPE_lasso, MSPE_ridge)
)

# Display the table using kable
kable(results_table, align = "cccc", caption = "Out of Sample Performance",
      format = "html")
```

```{r}

# Define custom function to format as currency without using currency function
set_price <- function(X) {
  formatted_price <- sprintf("$%.2f", exp(X))  # Format the exp(X) result as currency
  return(formatted_price)
}

#set_price <- function(X) {exp(X) |> scales::currency()}


### Compare TEST predictions
#test_preds <- cbind(seq(1:length(lm_pred)), 
#                    set_price(lm_pred),
#                    1,
#                    set_price(lm_step_pred), 
#                    set_price(lm_ridge_pred), 
#                    set_price(lm_lasso_pred),
#                    set_price(data_test$log_price)) |> as.data.table()
#test_preds <- test_preds[!is.na(V1)]
#setnames(test_preds, c("INDX", "LM", "LM_ROBUST", "STEPWISE", "RIDGE", "LASSO", "ACTUAL"))

#head(test_preds, 30)


#test_preds[,SSE_LM:=(LM-ACTUAL)^2]
#test_preds[,SSE_STEPWISE:=(STEPWISE-ACTUAL)^2]
#test_preds[,SSE_RIDGE:=(RIDGE-ACTUAL)^2]
#test_preds[,SSE_LASSO:=(LASSO-ACTUAL)^2]

### RMSE of predictions
# Based on test predictions, the ridge method wins!
#test_preds[,sqrt(sum(SSE_LM))/.N]
#test_preds[,sqrt(sum(SSE_STEPWISE))/.N]
#test_preds[,sqrt(sum(SSE_RIDGE))/.N]
#test_preds[,sqrt(sum(SSE_LASSO))/.N]

#Variable cleanup
#rm(all_amenities)
#rm(dest_points)
#rm(filtered_data)
#rm(null_model)
#rm(full_model)
#rm(tt_split)
#rm(cols)
#rm(inds)


### TO DO 
## Diagnostics
# Biggest residuals

#test_preds[order(-SSE_LM)][1:10]


# Outliers
#test_preds[LM==max(test_preds$LM)]
#data_orig_test[545]
#data_orig_test[652]
#data_orig_test[571]
```

Best model is OLS

# Section 5 - Discussion

Type
