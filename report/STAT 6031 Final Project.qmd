---
title: "STAT 6031 Final Project"
author: "Adriana Gonzalez Sanchez & Jack McMahon"
format:
  html:
    fig-align: center
    code-fold: true
    results: hide
    toc: true
    css: custom.css
    embed-resources: true
editor: visual
---

```{r, message = FALSE, warning = FALSE}

library(data.table)
library(ggplot2)
library(dplyr)
library(jsonlite)
library(reshape2)
library(knitr)
library(kableExtra)
library(caret)
library(MASS)
library(glmnet)
library(boot)
library(scales)
library(car)
```

# Section 1 - Data Description

The dataset for this project can be accessed via the following link: <https://insideairbnb.com/get-the-data/>, under section "New Orleans, Louisiana, United States" and under the file "listings.csv.gz."

It contains 7,118 observations and 75 covariates related to Airbnb listings in the New Orleans, Louisiana area. For a detailed description of each covariate, please refer to the following document: <https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit?gid=1322284596#gid=1322284596>.

![](New%20Orleans.jpg){fig-align="center" width="637"}

# Section 2 - Data Cleaning

Our analysis aims to predict Airbnb prices in the New Orleans area using various metrics, with "price" serving as the dependent variable. To ensure the quality and reliability of the data, we began by cleaning the dataset. This initial step involved several key tasks, including the removal of dollar signs from the price column and filtering out rows with missing price information.

Further data preparation involved parsing the amenities column, which was in JSON format, to identify potentially significant amenities that could influence price predictions. We also removed rows containing unrealistic price values or listings with zero availability, as these could skew the analysis.

```{r, message = FALSE, warning = FALSE}

raw_data <- fread("Listings_New_Orleans.csv")
bnb_data <- raw_data

### Remove listings with no price data
bnb_data <- bnb_data |> mutate(price = as.numeric(gsub("[$,]", "", as.character(price))))
bnb_data <- bnb_data[nchar(price)>1]  # (!!!) drops roughly 1000 records (!!!)

### Parse Amenities JSON

# Fix up quotes, so we can parse this JSON
bnb_data[,amenities:=gsub('""','"' , amenities, fixed=TRUE)]
bnb_data[,amenities_list:=lapply(bnb_data$amenities, function(x) {parse_json(x)})]

# Collapse list items into a table, for easier review of what these look like...
all_amenities <- unlist(bnb_data[, amenities_list])  |> data.table()
all_amenities <- all_amenities[,.N,by=V1][order(-N)]

# Potential dummy variables for "significant" amenities
bnb_data[amenities_list %like% "Long term stays allowed",has_long_term:=1]
bnb_data[amenities_list %like% "Backyard",has_backyard:=1]
bnb_data[amenities_list %like% "Patio",has_patio:=1]
bnb_data[amenities_list %like% "Pool",has_pool:=1]
bnb_data[amenities_list %like% "Stainless",has_stainless:=1]
bnb_data[amenities_list %like% "Lake",has_lake:=1]
bnb_data[amenities_list %like% "Private",has_privacy:=1]
bnb_data[amenities_list %like% "Window guards",has_window_guards:=1]
bnb_data[amenities_list %like% "Indoor fireplace",has_indoor_fireplace:=1]

cols <- names(bnb_data)[names(bnb_data) %like% "has" & !names(bnb_data) %in% c("host_has_profile_pic", "has_availability")]
bnb_data[ , (cols) := lapply(.SD, nafill, fill=0), .SDcols = cols]

### Remove "unrealized" listings - these have anomalous prices AND zero availability
# Or, not available within the next 365 days.....
# These are likely to have never had an actual stay and therefore their pricing is not relevant to our model
high_low <- bnb_data[,quantile(price, probs=c(.025, 0.975))]
bnb_data <- bnb_data[!(((price>high_low[2] | price<high_low[1]) & availability_30 == 0) | availability_365 == 0)]
  

```

Next, we proceeded to select the most relevant variables for our analysis. We removed columns containing URLs, IDs, names, descriptions, and overviews, as well as columns with neighborhood information, since we already had latitude and longitude data. Additionally, we eliminated columns with redundant information or those filled entirely with missing values (NAs).

After removing unnecessary columns, we ensured the data was in a workable format by standardizing missing values, coding them all as NA (as some were empty cells and others were "N/A"). We also converted the boolean columns from "True/False" to 1/0, where 1 corresponded to True and 0 to False. Furthermore, we removed percentage symbols from certain columns to convert them into numeric values.

To improve the utility of the data, we modified the "host_since" column to reflect the number of years the host had been active, making it easier to model. In addition to these adjustments, we incorporated spatial data by adding the locations of the top 10 destinations in the area, as listed by TripAdvisor, and created a new column to count the number of points of interest within a 1 km radius of each rental.

Below, you can observe two New Orleans maps, showing the spatial location of each Airbnb in the New Orleans area as well as the ones within 1 km of a top 10 TripAdvisor destination:

::: panel-tabset
## Airbnb Location

![](Listing%20Prices.jpeg){fig-align="center" width="532"}

## Top 10 Location

![](Top%2010%20Spots.jpeg){fig-align="center" width="529"}
:::

With these modifications completed, our \`\`final dataset'' contained 5,921 observations and 45 covariates. These cleaning and selection steps ensured that the dataset was both accurate and relevant for building a predictive model for Airbnb pricing in the New Orleans region.

```{r, message = FALSE, warning = FALSE}

# Select useful variables

bnb_data <- bnb_data %>% 
  dplyr::select(-listing_url, -scrape_id, -last_scraped, -source, -name,
                -description,-picture_url, -host_id, -host_url, -host_name,
                -host_about, -host_thumbnail_url, -host_picture_url,
                -bathrooms_text, -minimum_nights,- maximum_nights,
                -minimum_minimum_nights, -maximum_minimum_nights,
                -minimum_maximum_nights, -maximum_maximum_nights,
                -calendar_updated, -has_availability, -calendar_last_scraped,
                -first_review, -license, -neighborhood_overview,
                -host_verifications, -neighbourhood,-neighbourhood_group_cleansed,
                -property_type, -amenities,-host_listings_count,
                -host_neighbourhood, -last_review)

# Replace "N/A" and empty with NA in the entire dataset
bnb_data <- bnb_data %>%
  mutate(across(where(is.character), ~ na_if(., "N/A"))) %>%
  mutate(across(where(is.character), ~ na_if(., "")))

## Convert T/F to 1/0 --> true = 1, false = 0
bnb_data <- bnb_data %>%
  mutate(across(c(host_is_superhost, host_has_profile_pic, 
                  host_identity_verified, instant_bookable),
                ~ if_else(as.character(.) == "t", 1,
                          if_else(as.character(.) == "f", 0, NA_real_))))

# Modify host_since column to host for x years, easier to model
bnb_data <- bnb_data %>%
  mutate(host_since_year = gsub("host since ", "", host_since),
         host_since_year = as.Date(host_since_year),
         host_years = ifelse(!is.na(host_since_year),  
                             as.numeric(format(Sys.Date(), "%Y")) - as.numeric(format(host_since_year, "%Y")),NA)) %>%
  dplyr::select(-host_since, -host_since_year)

# Code host_location column as 1 if New Orleans, 0 otherwise
bnb_data <- bnb_data %>%
  mutate(host_location = if_else(host_location == "New Orleans, LA", 1, 0))

# Remove % symbol and convert to numeric
bnb_data$host_response_rate <- as.numeric(gsub("%", "",
                                               bnb_data$host_response_rate))
bnb_data$host_response_rate <- as.numeric(bnb_data$host_response_rate)

bnb_data$host_acceptance_rate <- as.numeric(gsub("%", "",
                                                 bnb_data$host_acceptance_rate))
bnb_data$host_acceptance_rate <- as.numeric(bnb_data$host_acceptance_rate)

## Remove calculated fields - these are duplicative
cols <- names(bnb_data)[names(bnb_data) %like% "calculated"]
bnb_data[ , (cols) := NULL]

### Add top 10 destination points from TripAdvisor
# This is meant to model how good the location of the BNB is....
# Represented as a count - so "6" means that the listing is within 1 km of 6 different attractions
### Add top 10 destination points from TripAdvisor
dest_points <- data.table("Rank" = seq(1:10),
                          "Location" = c("WW2 Museum", "French Quarter", "Frenchman Street", "Garden District", "Jackson Square", "Preservation Hall", "Mardi Gras World", "St. Louis Cemetary", "New Orleans City Park", "St. Louis Cathedral"),
                          "Lat" = c(29.943208928864262, 29.959424483411674, 29.964318181205662, 29.928845784748955, 29.95756024289618, 29.958610251707462, 29.941144698995455, 29.959608239041174, 29.993454400417903, 29.95815187771942),
                          "Long" = c(-90.07057952646183, -90.06491814669677, -90.05773360204412, -90.08380077303872,-90.06294615548948, -90.06534875344381, -90.0621223634231, -90.07117833809907, -90.09813780389588, -90.06370009040566))


### Find everything within 1 km of each point
# Non equi-join (https://www.r-bloggers.com/2021/02/the-unequalled-joy-of-non-equi-joins/)

# Get locations within 1 km of key points (Latitude only)
near_id <- data.table()

for (i in 1:nrow(dest_points)) {
  setkey(bnb_data, latitude)
  setkey(dest_points, Lat)
  close_lat <- dest_points[i][bnb_data, roll = .01, nomatch = NULL]
  
  
  # Now filter to within 1km - longitude
  setkey(close_lat, longitude)
  setkey(dest_points, Long)
  close_lat_long <- dest_points[i][close_lat, roll = 0.01, nomatch = NULL][,.(id)]
  
  near_id <- rbind(near_id, close_lat_long)
  rm(close_lat)
  rm(close_lat_long)
}

### Count up how many points of interest are within 1 km of a given rental
near_id <- as.data.table(near_id)
near_id <- near_id[,near_top_10 := .N, by=id]

### Remove some other columns
bnb_data <- bnb_data %>%
  dplyr::select(-maximum_nights_avg_ntm, -amenities_list)

# Add this new derived variable back into the main dataset
bnb_data <- near_id[bnb_data,on="id"]
bnb_data <- bnb_data[is.na(near_top_10), near_top_10:=0]|> unique()

rm(near_id)
rm(high_low)
rm(i)
rm(all_amenities)
rm(cols)

bnb_data <- bnb_data %>%
  dplyr::select(-id)

final_data <- bnb_data
```

# Section 3 - Exploratory Data Analysis

Once we had a dataset we could work with, our next step was to explore more in depth both binary and non-binary covariates.

For each non-binary covariate, we examined key statistics, including the mean, median, standard deviation, minimum, maximum, and the number of missing values. The table below summarizes these findings:

```{r, message = FALSE}

### Sample size
n <- nrow(final_data)
p <- ncol(final_data)

### Report summary statistics
summary_stats <- function(data)
  {
    results <- list()
    # Loop through each column in the dataset
    for (col in names(data)) 
      {
        # Check if the column is numeric and not binary
        unique_values <- unique(na.omit(data[[col]]))
        if (is.numeric(data[[col]]) && length(unique_values) > 2)
          {
            # Calculate summary statistics
            summary_stats <- c(Mean = round(mean(data[[col]], na.rm = TRUE), 3),
                               Median = round(median(data[[col]], na.rm = TRUE), 3),
                               SD = round(sd(data[[col]], na.rm = TRUE), 3),
                               Min = round(min(data[[col]], na.rm = TRUE), 3),
                               Max = round(max(data[[col]], na.rm = TRUE), 3),
                               NAs = sum(is.na(data[[col]])))
      
            # Store the results in a list
            results[[col]] <- summary_stats
          } 
        else 
          {
            message(paste("Skipping binary or non-numeric column:", col))
          }
      }
  
    # Convert the list to a data frame for easier viewing
    results_df <- as.data.frame(do.call(rbind, results))
    return(results_df)
  }

nonbinary_summary <- summary_stats(final_data)

```

```{r}

# Create a kable table and center the text in the cells
kable(nonbinary_summary, align = "rcccccc",
      caption = "Summary Statistics for Non-Binary Variables",
      col.names = c("Variable", "Mean", "Median", "Standard Deviation",
                    "Min", "Max", "NAs"),
      format = "html", escape = FALSE)
  

```

Regarding the binary variables, we looked at the percentage of 1s and 0s to better understand the distribution and prevalence of each category. This analysis helps us identify imbalances in the data, which could potentially affect model performance. By examining the proportions, we can determine if any variables exhibit skewed distributions. Additionally, understanding the distribution of binary variables provides insight into their relevance and potential impact on the outcome variable.

```{r}

### Report % of binary variables
report_binary_percentages <- function(data) {
  # Initialize an empty list to store results
  results <- list()
  
  # Loop through each column in the dataset
  for (col in names(data)) {
    # Check if the column is binary (contains only 0s, 1s, or NA values)
    unique_values <- unique(na.omit(data[[col]]))
    if (all(unique_values %in% c(0, 1))) {
      # Calculate the percentages of 0s and 1s, ignoring NAs
      percent_1 <- mean(data[[col]] == 1, na.rm = TRUE) * 100
      percent_0 <- mean(data[[col]] == 0, na.rm = TRUE) * 100
      
      # Store the results in a list
      results[[col]] <- c(`% of 1s` = percent_1, `% of 0s` = percent_0)
    }
  }
  
  # Convert the list to a data frame for easier viewing
  results_df <- as.data.frame(do.call(rbind, results))
  return(results_df)
}

binary_summary <- report_binary_percentages(final_data)

```

```{r}

# Create a kable table and center the text in the cells
kable(binary_summary, align = "ccccc",
      col.names = c("Variable", "% of 1s", "% of 0s"),
      caption = "Summary Statistics for Binary Variables",
      format = "html")

```

From the table above, we observe that most hosts provide information about their location, include profile pictures, and have verified identities. However, the majority of rentals lack amenities such as patios, backyards, pools, stainless steel appliances, lakeside views, or indoor fireplaces, among other features.

Our hope is that the fact these enhanced amenities are uncommon (as opposed to something common, such as Wifi) will provide a clear signal for predicting prices for more upscale listings.

### Distribution of Covariates

Reviewing our summary data, we noted that much of our data is skewed. To explore this skewness in more detail, we analyzed the distributions using boxplots and density plots:

```{r}

## Look at density plots and boxplots of non-binary variables

create_plots <- function(data) {
  # Loop through each column in the dataset
  for (col in names(data)) {
    # Check if the column is numeric and not binary (contains values other than 0 and 1)
    unique_values <- unique(na.omit(data[[col]]))
    if (is.numeric(data[[col]]) && length(unique_values) > 2) {
      # Create boxplot
      boxplot(data[[col]], main = paste("Boxplot"), xlab = col, col = "#5F9DC8")
      
      # Create density plot
      plot(density(data[[col]], na.rm = TRUE), main = paste("Density Plot"), xlab = col, col = "#5F9DC8")
    } 
  }
}

```

::: panel-tabset
## 1

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_1 <- final_data %>%
  dplyr::select(near_top_10, host_response_rate, host_acceptance_rate,
         host_total_listings_count)

create_plots(selected_columns_1)
```

## 2

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_2 <- final_data %>%
  dplyr::select(latitude, longitude, accommodates, bathrooms)

create_plots(selected_columns_2)
```

## 3

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_3 <- final_data %>%
  dplyr::select(bedrooms, beds, price, minimum_nights_avg_ntm)

create_plots(selected_columns_3)
```

## 4

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_4 <- final_data %>%
  dplyr::select(availability_30, availability_60, availability_90,
         availability_365)

create_plots(selected_columns_4)
```

## 5

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_5 <- final_data %>%
  dplyr::select(number_of_reviews, number_of_reviews_ltm,
         number_of_reviews_l30d, review_scores_rating)

create_plots(selected_columns_5)
```

## 6

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_6 <- final_data %>%
  dplyr::select(review_scores_accuracy, review_scores_cleanliness,
         review_scores_checkin, review_scores_communication)

create_plots(selected_columns_6)
```

## 7

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_7 <- final_data %>%
  dplyr::select(review_scores_location, review_scores_value,
         reviews_per_month, host_years)

create_plots(selected_columns_7)
```
:::

The distribution plots above highlighted a few concerns with the data that we sought to address: 1) Our dependent variable (price) is strongly skewed 2) most of our covariates are skewed and multimodal 3) a few covariates contain outliers, and 4) some of our covariates are highly correlated.

### Addressing Non-Normality

Taking a closer look at our dependent variable (price), we decided to log-transform our prices, given their extreme right skewness. As shown below, this step helped make the distribution of values for this variable more normal.

::: panel-tabset
## Before

```{r, message=FALSE}

ggplot(final_data, aes(price)) +
  geom_histogram(bins = 30, fill = "#5F9DC8", color = "black", alpha = 0.6) + 
  labs(title = "Distribution of AirBNB Prices - New Orleans", 
       x = "Price", y = "Count") +
  theme_bw()

```

## After

```{r, message=FALSE}

ggplot(final_data, aes(log(price))) +
  geom_histogram(bins = 30, fill = "#5F9DC8", color = "black", alpha = 0.6) +
  labs(title = "Distribution of log(AirBNB Prices) - New Orleans", 
       x = "Price", y = "Count") +
  theme_bw()
```
:::

### Addressing Outliers

We took a closer look at which columns contained outliers and how many, and the results are summarized in the table below:

```{r, message = FALSE}

# Example dataset
data <- final_data  # Replace with your actual dataset

# Initialize an empty data frame to store results
outliers_summary <- data.frame(Variable = character(), 
                               Num_Outliers = integer(), 
                               stringsAsFactors = FALSE)

# Loop through all columns in the dataset
for (col in colnames(data)) {
  
  # Check if the column is numeric and has more than two unique values
  if (is.numeric(data[[col]]) && length(unique(data[[col]])) > 2) {
    
    # Calculate the IQR (Interquartile Range)
    Q1 <- quantile(data[[col]], 0.25, na.rm = TRUE)
    Q3 <- quantile(data[[col]], 0.75, na.rm = TRUE)
    IQR_value <- Q3 - Q1
    
    # Define the lower and upper bounds for outliers
    lower_bound <- Q1 - 1.5 * IQR_value
    upper_bound <- Q3 + 1.5 * IQR_value
    
    # Identify outliers
    outliers <- data[[col]] < lower_bound | data[[col]] > upper_bound
    
    # If there are outliers, add the information to the data frame
    num_outliers <- sum(outliers, na.rm = TRUE)
    if (num_outliers > 0) {
      outliers_summary <- rbind(outliers_summary,
                                data.frame(Variable = col,
                                           Num_Outliers = num_outliers))
    }
  }
}

# Display the results using kable
kable(outliers_summary, align = "lc",
      caption = "Summary of Outliers in Variables", 
      col.names = c("Variable", "Number of Outliers"),
      format = "html")

```

As shown in the table above, many variables contained outliers. However, this was primarily due to some listings that could accommodate 6, 8, 10, or even 20 people.

Because each listing had a URL, we checked a handful these listings on the Airbnb website, and found that the for each "outlier" the data was valid (ie. it showed up that way on the website). Therefore, although our covariates included outliers, we decided to retain them, as they represented valid data.

### Addressing Multicollinearity

Other aspects we examined include correlations and multicollinearity. We consider two variables to be highly correlated if their correlation is 0.7 or greater. The table below summarizes the pairs of variables that exhibit strong correlations.

```{r}

numeric_data <- final_data %>%
  dplyr::select(-neighbourhood_cleansed, -room_type, -host_response_time,
                -latitude, -longitude)

numeric_data <- na.omit(numeric_data)

numeric_data <- as_tibble(numeric_data)

correlation_matrix <- cor(numeric_data)

threshold <- 0.7

# Find pairs of variables with correlation greater than the threshold
strong_correlations <- which(abs(correlation_matrix) > threshold, arr.ind = TRUE)

# Filter out the lower triangle (duplicate pairs)
strong_correlations <- strong_correlations[strong_correlations[, 1] < strong_correlations[, 2], ]

# Create a data frame to store variable pairs and their correlation values
correlation_data <- data.frame(
  Variable1 = character(),
  Variable2 = character(),
  Correlation = numeric(),
  stringsAsFactors = FALSE
)

# Populate the data frame with pairs and their correlation values
for (i in 1:nrow(strong_correlations)) {
  var1 <- rownames(correlation_matrix)[strong_correlations[i, 1]]
  var2 <- colnames(correlation_matrix)[strong_correlations[i, 2]]
  cor_value <- correlation_matrix[strong_correlations[i, 1], strong_correlations[i, 2]]
  correlation_data <- rbind(correlation_data, data.frame(Variable1 = var1, Variable2 = var2, Correlation = cor_value))
}

# Print the table using kable
kable(correlation_data, align = "ccc", 
      col.names = c("Variable 1", "Variable 2", "Correlation"),
      caption = "Strongly Correlated Variables")
```

To model the effect of the highly correlated variables in our model, we created an reduced dataset where we removed the most highly correlated variables.

In this dataset, we kept **accommodates** and removed **bathrooms**, **bedrooms**, and **beds** because **accommodates** is a broader variable that implicitly incorporates aspects such as the number of bedrooms, bathrooms, and beds. Similarly, we kept **review_scores_rating** and remove the other review categories, as they are highly correlated to the overall score.

We also kept **availability_365** and removed **availability_30**, **availability_60**, and **availability_90** because we are interested in non-seasonal availability, and we believe **availability_365** best represents this.

Furthermore, we chose to keep **reviews_per_month** instead of **number_of_reviews_ltm**, as it is a more directly interpretable measure of activity.

After removing these variables from our reduced dataset, we did not observe any significant correlation or multicollinearity.

```{r}

reduced_data <- numeric_data %>%
  dplyr::select(-bedrooms, -bathrooms, -beds, -availability_30, -availability_60,
                -availability_90, -review_scores_accuracy,
                -review_scores_cleanliness, -review_scores_checkin,
                -review_scores_communication, -review_scores_location,
                -review_scores_value, -number_of_reviews_ltm)

correlation_matrix <- cor(reduced_data)

threshold <- 0.7

strong_correlations <- which(abs(correlation_matrix) > threshold, arr.ind = TRUE)

vif_values <- vif(lm(reduced_data))
```

# Section 4 - Modeling

Before modeling our data, we took a few final steps to ensure our data was in the format required for regression analysis.

### Finalizing the Data

We set up dummy variables for our categorical measures. We also added an interaction variable (number_of_reviews \* review_score_rating) to our data, as we believe that it is possible that more good reviews may have non-linear impact on prices:

```{r, warning=FALSE}

set.seed(111)

setDT(reduced_data)

reduced_data <- reduced_data %>%
  mutate(logprice = log(price)) %>%
  dplyr::select(-price)

# Get unique levels of the 'host_response_time' variable
inds <- unique(reduced_data$host_response_time)

# Create dummy variables for each unique level in 'host_response_time'
for (x in inds) {
  reduced_data[, paste0("host_response_time_", x) := ifelse(host_response_time == x, 1, 0)]
}

inds <- unique(reduced_data$room_type)

# Create dummy variables for each unique level in 'host_response_time'
for (x in inds) {
  reduced_data[, paste0("host_response_time_", x) := ifelse(host_response_time == x, 1, 0)]
}

reduced_data[,host_owns_gt_5:=ifelse(host_total_listings_count > 5, 1, 0)]
reduced_data[,host_owns_2_5:=ifelse(host_total_listings_count < 5 & host_total_listings_count > 1, 1,0 )]
reduced_data[,host_total_listings_count:=NULL]

# Add interaction variable for # of reviews * review 
# we suspect that the number of high review scores might matter
reduced_data[,review_score_ct_interaction:=review_scores_rating * number_of_reviews]

### Keep a reference dataset
model_data <- copy(reduced_data)
```

The dataset was split into a training set (80%) and a testing set (20%) to facilitate model selection and prediction analysis. We then did our log-transformation of price, to improve normality of our dependent variable:

```{r}

### Set up train/test split
tt_split <- createDataPartition(
  y = model_data$logprice,
  p = .80, ## The percentage of data in the training set
  list = FALSE
)

data_train <- model_data[ tt_split,]
data_test <- model_data[ -tt_split,]
```

After carefully examining the covariates, we proceeded to the modeling phase.

### Modeling - Reduced Dataset (no multicollinearity)

We initially started with a simple linear model, but also created models using stepwise selection, Lasso, and Ridge regression to evaluate whether any of them could enhance the in-sample fit.

The residuals for each model were extremely similar, so we decided to choose our model based off MSPE (mean squared prediction error) from our hold-out sample of data.

::: panel-tabset
## LM

```{r}

simple_lm <- lm(logprice~.,data=data_train)

par(mfrow = c(2,2))
plot(simple_lm)
```

## Stepwise

```{r}

null_model <- lm(logprice ~ 1, data = data_train[complete.cases(data_train)])
full_model <- lm(logprice ~ ., data = data_train[complete.cases(data_train)])

step_model <- step(null_model, scope = list(lower = null_model, 
                                         upper = full_model),
                       k = log(nrow(data_train)), trace = F)

par(mfrow = c(2,2))
plot(step_model)

```

## Lasso

```{r}

lasso_cv <- cv.glmnet(data_train[, !"logprice", with=FALSE] |> as.matrix(),
                         data_train[,logprice] |> as.matrix(),
                         alpha=1,
                         nfolds=10)
par(mfrow = c(2,2))
#plot(lm_lasso_cv)

lasso_model <- glmnet(data_train[, !"logprice", with=FALSE] |> as.matrix(),
                      data_train[,logprice] |> as.matrix(),
                      alpha=1,
                      lambda = lasso_cv$lambda.min)

lasso_fitted <- predict(lasso_cv,
                        newx = data_train[, !"logprice", with = FALSE] |> as.matrix(), 
                       s = lasso_cv$lambda.min)

# Compute residuals
lasso_residuals <- data_train[, logprice] - lasso_fitted

# Compute standardized residuals
std_residuals_lasso <- lasso_residuals / sd(lasso_residuals)

# 1. Residuals vs Fitted
plot(lasso_fitted, lasso_residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

# 2. Normal Q-Q Plot
qqnorm(lasso_residuals, main = "Normal Q-Q")
qqline(lasso_residuals, col = "red", lty = 2)

# 3. Scale-Location Plot
plot(lasso_fitted, sqrt(abs(std_residuals_lasso)),
     main = "Scale-Location",
     xlab = "Fitted Values",
     ylab = expression(sqrt("|Standardized residuals|")),
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

# 4. Residuals vs Leverage
# Leverage and Cook's distance are not directly available from glmnet.
# Approximate by treating leverage as the influence of fitted values.

# Approximation: Simulate leverage-like values (optional)
lasso_leverages <- hatvalues(lm(data_train[, logprice] ~ as.matrix(data_train[, !"logprice", with = FALSE])))

plot(lasso_leverages, lasso_residuals, 
     main = "Residuals vs Leverage",
     xlab = "Leverage",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

```

## Ridge

```{r}

ridge_cv <- cv.glmnet(data_train[, !"logprice", with=FALSE] |> as.matrix(),
                         data_train[,logprice] |> as.matrix(),
                         alpha=0,
                         nfolds=10)

#coef(lm_ridge_cv, s=lm_ridge_cv$lambda.min)
par(mfrow = c(2,2))
#plot(lm_ridge_cv)

ridge_model <- glmnet(data_train[, !"logprice", with=FALSE] |> as.matrix(),
                      data_train[,logprice] |> as.matrix(),
                      alpha=0,
                      lambda = ridge_cv$lambda.min)

ridge_fitted <- predict(ridge_cv,
                        newx = data_train[, !"logprice", with = FALSE] |> as.matrix(), 
                       s = ridge_cv$lambda.min)

# Compute residuals
ridge_residuals <- data_train[, logprice] - ridge_fitted

# Compute standardized residuals
std_residuals_ridge <- ridge_residuals / sd(ridge_residuals)


# 1. Residuals vs Fitted
plot(ridge_fitted, ridge_residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

# 2. Normal Q-Q Plot
qqnorm(ridge_residuals, main = "Normal Q-Q")
qqline(ridge_residuals, col = "red", lty = 2)

# 3. Scale-Location Plot
plot(ridge_fitted, sqrt(abs(std_residuals_ridge)),
     main = "Scale-Location",
     xlab = "Fitted Values",
     ylab = expression(sqrt("|Standardized residuals|")),
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

# 4. Residuals vs Leverage
# Leverage and Cook's distance are not directly available from glmnet.
# Approximate by treating leverage as the influence of fitted values.

# Approximation: Simulate leverage-like values (optional)
ridge_leverages <- hatvalues(lm(data_train[, logprice] ~ as.matrix(data_train[, !"logprice", with = FALSE])))

plot(ridge_leverages, ridge_residuals, 
     main = "Residuals vs Leverage",
     xlab = "Leverage",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)
```
:::

Our models also had similar MSPE of around 0.195, which indicates that our models performed roughly the same as one another in accurately predicting prices:

```{r}

# In-sample average sum squared error (ASE)

ASE_lm <- mean(simple_lm$residuals^2)
MSE_lm <- summary(simple_lm)$sigma^2

ASE_step <- mean(step_model$residuals^2)
MSE_step <- summary(step_model)$sigma^2

lasso_residuals <- data_train[, logprice] - lasso_fitted
ASE_lasso <- mean(lasso_residuals^2)

non_zero_coefs <- sum(coef(lasso_model, s = lasso_cv$lambda.min) != 0)
df_lasso <- nrow(data_train) - non_zero_coefs  
RSS_lasso <- sum(lasso_residuals^2)
MSE_lasso <- RSS_lasso / df_lasso

ridge_residuals <- data_train[, logprice] - ridge_fitted
ASE_ridge <- mean(ridge_residuals^2)

sse_values_ridge <- sum((ridge_residuals)^2)

# Remove the 'logprice' column and convert to matrix
X <- data_train[, !"logprice", with=FALSE] |> as.matrix()

best_lambda_ridge <- ridge_cv$lambda.min
hat_matrix_ridge <- X %*% solve(t(X) %*% X + 
                                  best_lambda_ridge * diag(ncol(X))) %*% t(X)
edf_ridge <- sum(diag(hat_matrix_ridge)) # EDF (Model DF) 
df_ridge <- nrow(data_train) - edf_ridge # Residuals DF
    
MSE_ridge <- sse_values_ridge/df_ridge

results_table <- tibble(
  Model = c("OLS", "Stepwise Regression", "Lasso", "Ridge"),
  ASE = c(ASE_lm, ASE_step, ASE_lasso, ASE_ridge),
  MSE = c(MSE_lm, MSE_step, MSE_lasso, MSE_ridge)
)

# Display the table using kable
kable(results_table, align = "cccc", caption = "Model Comparison: ASE and MSE",
      format = "html")
```

Here is the out-of-sample performance for price prediction:

```{r}

x_test <- data_test %>%
  dplyr::select(-logprice)

y_hat_test_ols <- predict(simple_lm, x_test)
MSPE_lm <- mean((data_test$logprice - y_hat_test_ols)^2)

y_hat_test_step <- predict(step_model, x_test)
MSPE_step <- mean((data_test$logprice - y_hat_test_step)^2)

x_test <- as.matrix(x_test)
y_hat_test_lasso <- predict(lasso_model, x_test,
                            s = lasso_cv$lambda.min)
MSPE_lasso <- mean((data_test$logprice - y_hat_test_lasso)^2)

y_hat_test_ridge <- predict(ridge_model, x_test, s = ridge_cv$lambda.min)
MSPE_ridge <- mean((data_test$logprice - y_hat_test_ridge)^2)

results_table <- tibble(
  Model = c("OLS", "Stepwise Regression", "Lasso", "Ridge"),
  MSPE = c(MSPE_lm, MSPE_step, MSPE_lasso, MSPE_ridge)
)

# Display the table using kable
kable(results_table, align = "cccc", caption = "Out of Sample Performance",
      format = "html")
```

When selecting a model, there is always a trade-off between performance and interpretability. Removing highly correlated variables improves interpretability but may not guarantee better predictive performance. For this reason, we also sought to compare the behavior of different models when using the full set of predictors, including those that are highly correlated.

### Modeling - Full Dataset (w/ multicollinearity)

To conduct the analysis, we followed the same steps as previously outlined with our full dataset. Therefore, all of the correlated columns removed during the EDA step were added back into the model. The results are presented below:

```{r}

final_data[,c("neighbourhood_cleansed","latitude", "longitude"):=NULL]

# Get rid of rows with NAs
final_data <- na.omit(final_data)

# Create Dummy Variables
inds <- unique(final_data$host_response_time)
final_data[, (inds) := lapply(inds, function(x) 
  {ifelse(host_response_time == x, 1,0)})]

final_data[,host_response_time:=NULL] ### Drop original column
final_data[,`a few days or more`:=NULL] ### To avoid perfect collinearity

inds <- unique(final_data$room_type)
final_data[, (inds) := lapply(inds, function(x) 
  {ifelse(room_type == x, 1, 0)})]

final_data[,room_type:=NULL] ### Drop original column
final_data[,`Shared room`:=NULL] ### To avoid perfect collinearity

final_data[,host_owns_gt_5:=ifelse(host_total_listings_count > 5, 1, 0)]
final_data[,host_owns_2_5:=ifelse(host_total_listings_count < 5 & host_total_listings_count > 1, 1,0 )]
final_data[,host_total_listings_count:=NULL]

# Add interaction variable for # of reviews * review 
# we suspect that the number of high review scores might matter
final_data[,review_score_ct_interaction:=review_scores_rating * number_of_reviews]

### Keep a reference dataset
full_model_data <- copy(final_data)

full_model_data[,log_price := log(price)]
full_model_data[,price:=NULL]

set.seed(111) 

tt_split <- createDataPartition(
  y = full_model_data$log_price,
  p = .80, ## The percentage of data in the training set
  list = FALSE
)

full_data_train <- full_model_data[ tt_split,]
full_data_test <- full_model_data[ -tt_split,]


```

::: panel-tabset
## LM

```{r}

simple_lm <- lm(log_price~.,data = full_data_train)
par(mfrow = c(2,2))
plot(simple_lm)
```

## Stepwise

```{r}

null_model <- lm(log_price ~ 1, data = full_data_train[complete.cases(full_data_train)])
full_model <- lm(log_price ~ ., data = full_data_train[complete.cases(full_data_train)])
step_model <- step(null_model, scope = list(lower = null_model, 
                                         upper = full_model),
                       k = log(nrow(full_data_train)), trace = F)
par(mfrow = c(2,2))
plot(step_model)

```

## Lasso

```{r}

lasso_cv <- cv.glmnet(full_data_train[, !"log_price", with=FALSE] |> as.matrix(),
                         full_data_train[,log_price] |> as.matrix(),
                         alpha=1,
                         nfolds=10)
par(mfrow = c(2,2))
#plot(lm_lasso_cv)
lasso_model <- glmnet(full_data_train[, !"log_price", with=FALSE] |> as.matrix(),
                      full_data_train[,log_price] |> as.matrix(),
                      alpha=1,
                      lambda = lasso_cv$lambda.min)
lasso_fitted <- predict(lasso_cv,
                        newx = full_data_train[, !"log_price", with = FALSE] |> as.matrix(), 
                       s = lasso_cv$lambda.min)
# Compute residuals
lasso_residuals <- full_data_train[, log_price] - lasso_fitted
# Compute standardized residuals
std_residuals_lasso <- lasso_residuals / sd(lasso_residuals)
# 1. Residuals vs Fitted
plot(lasso_fitted, lasso_residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)
# 2. Normal Q-Q Plot
qqnorm(lasso_residuals, main = "Normal Q-Q")
qqline(lasso_residuals, col = "red", lty = 2)
# 3. Scale-Location Plot
plot(lasso_fitted, sqrt(abs(std_residuals_lasso)),
     main = "Scale-Location",
     xlab = "Fitted Values",
     ylab = expression(sqrt("|Standardized residuals|")),
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)
# 4. Residuals vs Leverage
# Leverage and Cook's distance are not directly available from glmnet.
# Approximate by treating leverage as the influence of fitted values.
# Approximation: Simulate leverage-like values (optional)
lasso_leverages <- hatvalues(lm(full_data_train[, log_price] ~ as.matrix(full_data_train[, !"log_price", with = FALSE])))
plot(lasso_leverages, lasso_residuals, 
     main = "Residuals vs Leverage",
     xlab = "Leverage",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

```

## Ridge

```{r}

ridge_cv <- cv.glmnet(full_data_train[, !"log_price", with=FALSE] |> as.matrix(),
                        full_data_train[,log_price] |> as.matrix(),
                         alpha=0,
                         nfolds=10)
#coef(lm_ridge_cv, s=lm_ridge_cv$lambda.min)
par(mfrow = c(2,2))
#plot(lm_ridge_cv)
ridge_model <- glmnet(full_data_train[, !"log_price", with=FALSE] |> as.matrix(),
                      full_data_train[,log_price] |> as.matrix(),
                      alpha=0,
                      lambda = ridge_cv$lambda.min)
ridge_fitted <- predict(ridge_cv,
                        newx = full_data_train[, !"log_price", with = FALSE] |> as.matrix(), 
                       s = ridge_cv$lambda.min)
# Compute residuals
ridge_residuals <- full_data_train[, log_price] - ridge_fitted
# Compute standardized residuals
std_residuals_ridge <- ridge_residuals / sd(ridge_residuals)
# 1. Residuals vs Fitted
plot(ridge_fitted, ridge_residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)
# 2. Normal Q-Q Plot
qqnorm(ridge_residuals, main = "Normal Q-Q")
qqline(ridge_residuals, col = "red", lty = 2)
# 3. Scale-Location Plot
plot(ridge_fitted, sqrt(abs(std_residuals_ridge)),
     main = "Scale-Location",
     xlab = "Fitted Values",
     ylab = expression(sqrt("|Standardized residuals|")),
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)
# 4. Residuals vs Leverage
# Leverage and Cook's distance are not directly available from glmnet.
# Approximate by treating leverage as the influence of fitted values.
# Approximation: Simulate leverage-like values (optional)
ridge_leverages <- hatvalues(lm(full_data_train[, log_price] ~ as.matrix(full_data_train[, !"log_price", with = FALSE])))
plot(ridge_leverages, ridge_residuals, 
     main = "Residuals vs Leverage",
     xlab = "Leverage",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

```
:::

```{r, warning = FALSE}

# In-sample average sum squared error (ASE)

ASE_lm <- mean(simple_lm$residuals^2)
MSE_lm <- summary(simple_lm)$sigma^2

ASE_step <- mean(step_model$residuals^2)
MSE_step <- summary(step_model)$sigma^2

lasso_residuals <- full_data_train[, log_price] - lasso_fitted
ASE_lasso <- mean(lasso_residuals^2)

non_zero_coefs <- sum(coef(lasso_model, s = lasso_cv$lambda.min) != 0)
df_lasso <- nrow(full_data_train) - non_zero_coefs  
RSS_lasso <- sum(lasso_residuals^2)
MSE_lasso <- RSS_lasso / df_lasso

ridge_residuals <- full_data_train[, log_price] - ridge_fitted
ASE_ridge <- mean(ridge_residuals^2)

sse_values_ridge <- sum((ridge_residuals)^2)

# Remove the 'logprice' column and convert to matrix
X <- full_data_train[, !"log_price", with=FALSE] |> as.matrix()

best_lambda_ridge <- ridge_cv$lambda.min
hat_matrix_ridge <- X %*% solve(t(X) %*% X + 
                                  best_lambda_ridge * diag(ncol(X))) %*% t(X)
edf_ridge <- sum(diag(hat_matrix_ridge)) # EDF (Model DF) 
df_ridge <- nrow(full_data_train) - edf_ridge # Residuals DF
    
MSE_ridge <- sse_values_ridge/df_ridge

results_table <- tibble(
  Model = c("OLS", "Stepwise Regression", "Lasso", "Ridge"),
  ASE = c(ASE_lm, ASE_step, ASE_lasso, ASE_ridge),
  MSE = c(MSE_lm, MSE_step, MSE_lasso, MSE_ridge)
)

# Display the table using kable
kable(results_table, align = "cccc", caption = "Model Comparison: ASE and MSE",
      format = "html")
```

```{r}

x_test <- full_data_test %>%
  dplyr::select(-log_price)

y_hat_test_ols <- predict(simple_lm, x_test)
MSPE_lm <- mean((full_data_test$log_price - y_hat_test_ols)^2)

y_hat_test_step <- predict(step_model, x_test)
MSPE_step <- mean((full_data_test$log_price - y_hat_test_step)^2)

x_test <- as.matrix(x_test)
y_hat_test_lasso <- predict(lasso_model, x_test,
                            s = lasso_cv$lambda.min)
MSPE_lasso <- mean((full_data_test$log_price - y_hat_test_lasso)^2)

y_hat_test_ridge <- predict(ridge_model, x_test, s = ridge_cv$lambda.min)
MSPE_ridge <- mean((full_data_test$log_price - y_hat_test_ridge)^2)

results_table <- tibble(
  Model = c("OLS", "Stepwise Regression", "Lasso", "Ridge"),
  MSPE = c(MSPE_lm, MSPE_step, MSPE_lasso, MSPE_ridge)
)

# Display the table using kable
kable(results_table, align = "cccc", caption = "Out of Sample Performance",
      format = "html")
```

As shown above, when using the model with correlated predictors, all methods demonstrate improved predictive performance. Additionally, because the MSPE is very similar to the model's MSE value, we see no indications of our model over-fitting the training data.

Since the goal of our project is to predict Airbnb prices in the New Orleans area, we have decided to use the model with correlated predictors, as it yields better predictive results.

### The Winner: Lasso

We chose the Lasso model to be our final price prediction model because it had the lowest MSPE of all of our models and was more parsimonious than models of comparable accuracy.

This model fit the data using 36 of 48 variables supplied to it, and achieved an MSPE of 0.1586327.

# Section 5 - Discussion

Our model revealed that the most salient factors that determine an Airbnb listing's price in this market were:

-   **is_Hotel** - It was interesting to see this as our biggest driver of costs, being nearly twice as big as the next largest factor. Our interpretation is that 1) hotels likely have a higher operating costs, and pass that along to the consumer and 2) they might only list rooms at an artificially high price, just to cash in when there's a big event, such as Mardi Gras. What this tells us is that hotel listings on Airbnb are likely to be overpriced, relative to the amenities you get.

-   **review_scores_location** - Location clearly affected the value of a listing, but our model clearly shows that **the review scores about the location** matter far more than proximity to TripAdvisor destinations. This could indicate there are other attractions (such as restaurants/bars) that our location variable did not adequately capture. This was a very interesting finding.

-   **has_pool** - this indicator came from the Amenities of each listing, and shows that people clearly value having an outdoor space to relax. Note: there may be some confounding of this variable with **is_Hotel**, since private residences are unlikely to have pools, while hotels often do.

-   **is_private_room** - this room type indicates listings where the room is part of the host's house (ie. they are sharing the house with the host). Not surprisingly, this type of listing was associated with lower listing prices.

-   **bathrooms** - While this seems like a strange variable for the model to key in on, the number of bathrooms seems to provide the most reliable estimator of how many people a residence can (comfortably) host. Our data had alternate measures like "Bedrooms", "Beds", and "Accomodates", but we suspected that some hosts inflate these totals to make their listing seem bigger/more valuable. Apparently, it is harder to inflate the number of bathrooms you have!

While our model did provide some surprising findings, the top factors did make sense. Other significant factors included:

-   Review score (overall)

-   Review score (check-in)

-   Review score (cleanliness)

-   Response time (within a day)

-   Host owning \> 5 properties on Airbnb

Our TripAdvisor variable was also retained in the Lasso model, but had a slightly smaller coefficient than these variables.

Our pricing model did a solid job of predicting the price of Airbnb listings in New Orleans, with moderately low prediction errors. A noticeable chunk of our residuals were driven by only a few VERY wrong predictions:

```{r}

data_test[,ACTUAL_PRICE:=exp(logprice)]
data_test[,LASSO_PRED:=exp(predict(lasso_model, x_test,
                               s = lasso_cv$lambda.min))]
data_test[,LASSO_ERROR:=(data_test$logprice - y_hat_test_lasso)^2]
data_test[,PRICE_DIFF:=exp(logprice) - LASSO_PRED]

#data_test[PRICE_DIFF == min(PRICE_DIFF), .(accommodates, bathrooms, beds, review_scores_rating, LASSO_PRED, ACTUAL_PRICE)]
```

#### Test Prediction Residuals

::: panel-tabset
## Histogram

Nearly all pricing errors in our model were very small - only a few observations were badly misclassified. These large residuals may be tied to the outliers we saw during our EDA:

```{r, warning=FALSE, message=FALSE}

library(ggplot2)

ggplot(data_test, aes(PRICE_DIFF)) + 
  geom_histogram(binwidth = 50, fill = "#5F9DC8", color = "black", 
                 alpha = 0.7) + 
  labs(title = "Distribution of AirBNB Prices - New Orleans", 
       x = "Price", y = "Count") +
  theme_bw() +
  theme(plot.title = element_text(hjust = 0.5), # Center the title
        axis.title = element_text(size = 12), # Adjust axis title size
        axis.text = element_text(size = 10)) # Adjust axis text size

#quantile(data_test$PRICE_DIFF, probs = seq(0, 1, 1/5))


```

## Map

Our model didn't appear to systematically over/underestimate prices in specific areas either.

This suggests that our model could reasonably accurately account for the variation in price data due to location differences:

![](Residual_Error_Test.jpeg)
:::

For your "average" Airbnb listing, our model could predict the price reasonably well: for most listings, we were able to predict the price within Â±\$50. Therefore, we believe we've modeled this system successfully.

## Further Study

**Generalizing for other markets**: This model could easily be applied to Airbnb rentals from other markets, to assess how well it describes trends in those markets. By re-training across multiple markets, this model could be generalized to reliably predict prices of ANY Airbnb listing.

**Data Supplementation**: Our model could also benefit from adding publicly-available data to improve predictions. There were many other potential variables related to location that likely impact prices - CRIME\*, transit, local housing prices.....Many of these variables have a publicly-available data source (such as the police department) to find this data. If we had more time, we would have liked to follow up on at least some of these factors to assess their impact on pricing.
