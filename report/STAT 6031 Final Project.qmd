---
title: "STAT 6031 Final Project"
author: "Adriana Gonzalez Sanchez & Jack McMahon"
format:
  html:
    fig-align: center
    code-fold: true
    results: hide
    toc: true
    css: custom.css
    embed-resources: true
editor: visual
---

```{r, message = FALSE, warning = FALSE}

library(data.table)
library(ggplot2)
library(dplyr)
library(jsonlite)
library(reshape2)
library(knitr)
library(kableExtra)
library(caret)
library(MASS)
library(glmnet)
library(boot)
library(scales)
```

# Section 1 - Data Description

The dataset for this project can be accessed via the following link: <https://insideairbnb.com/get-the-data/>, under section "New Orleans, Louisiana, United States" and under the file "listings.csv.gz."

It contains 7,118 observations and 75 covariates related to Airbnb listings in the New Orleans, Louisiana area. For a detailed description of each covariate, please refer to the following document: <https://docs.google.com/spreadsheets/d/1iWCNJcSutYqpULSQHlNyGInUvHg2BoUGoNRIGa6Szc4/edit?gid=1322284596#gid=1322284596>.

![](New%20Orleans.jpg){fig-align="center" width="637"}

# Section 2 - Data Cleaning

Our analysis aims to predict Airbnb prices in the New Orleans area using various metrics, with "price" serving as the dependent variable. To ensure the quality and reliability of the data, we began by cleaning the dataset. This initial step involved several key tasks:

1.  **Removing invalid data:** All Airbnb listings missing a price were removed from the dataset. We also removed listings with zero availability, as these tended to have more unrealistic prices (one was \$28,000 per night) which could skew the analysis.

    -   Our analysis therefore seeks to model the **market price** of Airbnb listings (ie. what price people are actually renting Airbnbs for), not the **list price**

    -   For example, I could list an Airbnb for \$1M, but if nobody is renting it, is that price meaningful?

```{r, message = FALSE, warning = FALSE}

raw_data <- fread("Listings_New_Orleans.csv")
bnb_data <- raw_data

### Remove listings with no price data
bnb_data <- bnb_data |> mutate(price = as.numeric(gsub("[$,]", "", as.character(price))))
bnb_data <- bnb_data[nchar(price)>1]  # (!!!) drops roughly 1000 records (!!!)

### Remove "unrealized" listings - these have anomalous prices AND zero availability
# Or, not available within the next 365 days.....
# These are likely to have never had an actual stay and therefore their pricing is not relevant to our model
high_low <- bnb_data[,quantile(price, probs=c(.025, 0.975))]
bnb_data <- bnb_data[!(((price>high_low[2] | price<high_low[1]) & availability_30 == 0) | availability_365 == 0)]
```

2.  **Conforming the data:** we parsed out dollar signs from the price column. We also unnested the amenities column, which was in JSON format, to identify potentially significant amenities that could influence price predictions.

```{r, message = FALSE, warning = FALSE}

### Parse Amenities JSON

# Fix up quotes, so we can parse this JSON
bnb_data[,amenities:=gsub('""','"' , amenities, fixed=TRUE)]
bnb_data[,amenities_list:=lapply(bnb_data$amenities, function(x) {parse_json(x)})]

# Collapse list items into a table, for easier review of what these look like...
all_amenities <- unlist(bnb_data[, amenities_list])  |> data.table()
all_amenities <- all_amenities[,.N,by=V1][order(-N)]

# Potential dummy variables for "significant" amenities
bnb_data[amenities_list %like% "Long term stays allowed",has_long_term:=1]
bnb_data[amenities_list %like% "Backyard",has_backyard:=1]
bnb_data[amenities_list %like% "Patio",has_patio:=1]
bnb_data[amenities_list %like% "Pool",has_pool:=1]
bnb_data[amenities_list %like% "Stainless",has_stainless:=1]
bnb_data[amenities_list %like% "Lake",has_lake:=1]
bnb_data[amenities_list %like% "Private",has_privacy:=1]
bnb_data[amenities_list %like% "Window guards",has_window_guards:=1]
bnb_data[amenities_list %like% "Indoor fireplace",has_indoor_fireplace:=1]
```

3.  **Variable Selection** - Next, we proceeded to select the most relevant variables for our analysis.

    -   We removed columns that had no clear relationship to the price (things like URLs, IDs, names).

    -   We eliminated columns with redundant information or those filled entirely with missing values (NAs).

    -   We also dropped text fields such as descriptions and overviews. Since major features for each Bnb listing were often tagged in the Amenities, these free-text fields did not seem to provide much new insight from what we already had.

    -   After parsing out the Amenities, we found over 2,000 possible indicators that could potentially serve as dummy variables. We narrowed this list down to only look at items present in at least 1% of listings. From this reduced list (\~150 possibilities), we picked 9 features that seemed the most clearly related to how desirable a property might be ("has_pool", for instance))

        ```{r, message = FALSE, warning = FALSE}

        # Select useful variables

        bnb_data <- bnb_data %>% 
          dplyr::select(-listing_url, -scrape_id, -last_scraped, -source, -name,
                        -description,-picture_url, -host_id, -host_url, -host_name,
                        -host_about, -host_thumbnail_url, -host_picture_url,
                        -bathrooms_text, -minimum_nights,- maximum_nights,
                        -minimum_minimum_nights, -maximum_minimum_nights,
                        -minimum_maximum_nights, -maximum_maximum_nights,
                        -calendar_updated, -has_availability, -calendar_last_scraped,
                        -first_review, -license, -neighborhood_overview,
                        -host_verifications, -neighbourhood,-neighbourhood_group_cleansed,
                        -property_type, -amenities,-host_listings_count,
                        -host_neighbourhood, -last_review)

        # Replace "N/A" and empty with NA in the entire dataset
        bnb_data <- bnb_data %>%
          mutate(across(where(is.character), ~ na_if(., "N/A"))) %>%
          mutate(across(where(is.character), ~ na_if(., "")))

        ## Convert T/F to 1/0 --> true = 1, false = 0
        bnb_data <- bnb_data %>%
          mutate(across(c(host_is_superhost, host_has_profile_pic, 
                          host_identity_verified, instant_bookable),
                        ~ if_else(as.character(.) == "t", 1,
                                  if_else(as.character(.) == "f", 0, NA_real_))))

        # Modify host_since column to host for x years, easier to model
        bnb_data <- bnb_data %>%
          mutate(host_since_year = gsub("host since ", "", host_since),
                 host_since_year = as.Date(host_since_year),
                 host_years = ifelse(!is.na(host_since_year),  
                                     as.numeric(format(Sys.Date(), "%Y")) - as.numeric(format(host_since_year, "%Y")),NA)) %>%
          dplyr::select(-host_since, -host_since_year)

        # Code host_location column as 1 if New Orleans, 0 otherwise
        bnb_data <- bnb_data %>%
          mutate(host_location = if_else(host_location == "New Orleans, LA", 1, 0))

        # Remove % symbol and convert to numeric
        bnb_data$host_response_rate <- as.numeric(gsub("%", "",
                                                       bnb_data$host_response_rate))
        bnb_data$host_response_rate <- as.numeric(bnb_data$host_response_rate)

        bnb_data$host_acceptance_rate <- as.numeric(gsub("%", "",
                                                         bnb_data$host_acceptance_rate))
        bnb_data$host_acceptance_rate <- as.numeric(bnb_data$host_acceptance_rate)

        ## Remove calculated fields - these are duplicative
        cols <- names(bnb_data)[names(bnb_data) %like% "calculated"]
        bnb_data[ , (cols) := NULL]


        ### Remove some other columns
        bnb_data <- bnb_data %>%
          dplyr::select(-maximum_nights_avg_ntm, -amenities_list)

        cols <- names(bnb_data)[names(bnb_data) %like% "has" & !names(bnb_data) %in% c("host_has_profile_pic", "has_availability")]
        bnb_data[ , (cols) := lapply(.SD, nafill, fill=0), .SDcols = cols]

        ```

4.  **Standardizing the Data:** After removing unnecessary columns, we ensured the data was in a workable format by standardizing missing values, coding them all as NA (as some were empty cells and others were "N/A"). We also converted the boolean columns from "True/False" to 1/0, where 1 corresponded to True and 0 to False. Furthermore, we removed percentage symbols from certain columns to convert them into numeric values. To improve the utility of the data, we modified the "host_since" column from a date to a numeric ("number of years active"), making it easier to model.

5.  **Adding supplementary data:** To model how location affects price, we added a new variable to model the "desirability" of a given location...We did this by grabbing the location (lat-long) of the top 10 TripAdvisor destinations in New Orleans, and created a new variable to count the number of points of interest within a 1 km radius of each rental.

    ```{r, message = FALSE, warning = FALSE}
    ### Add top 10 destination points from TripAdvisor
    # This is meant to model how good the location of the BNB is....
    # Represented as a count - so "6" means that the listing is within 1 km of 6 different attractions
    ### Add top 10 destination points from TripAdvisor
    dest_points <- data.table("Rank" = seq(1:10),
                              "Location" = c("WW2 Museum", "French Quarter", "Frenchman Street", "Garden District", "Jackson Square", "Preservation Hall", "Mardi Gras World", "St. Louis Cemetary", "New Orleans City Park", "St. Louis Cathedral"),
                              "Lat" = c(29.943208928864262, 29.959424483411674, 29.964318181205662, 29.928845784748955, 29.95756024289618, 29.958610251707462, 29.941144698995455, 29.959608239041174, 29.993454400417903, 29.95815187771942),
                              "Long" = c(-90.07057952646183, -90.06491814669677, -90.05773360204412, -90.08380077303872,-90.06294615548948, -90.06534875344381, -90.0621223634231, -90.07117833809907, -90.09813780389588, -90.06370009040566))


    ### Find everything within 1 km of each point
    # Non equi-join (https://www.r-bloggers.com/2021/02/the-unequalled-joy-of-non-equi-joins/)

    # Get locations within 1 km of key points (Latitude only)
    near_id <- data.table()

    for (i in 1:nrow(dest_points)) {
      setkey(bnb_data, latitude)
      setkey(dest_points, Lat)
      close_lat <- dest_points[i][bnb_data, roll = .01, nomatch = NULL]
      
      
      # Now filter to within 1km - longitude
      setkey(close_lat, longitude)
      setkey(dest_points, Long)
      close_lat_long <- dest_points[i][close_lat, roll = 0.01, nomatch = NULL][,.(id)]
      
      near_id <- rbind(near_id, close_lat_long)
      rm(close_lat)
      rm(close_lat_long)
    }

    ### Count up how many points of interest are within 1 km of a given rental
    near_id <- as.data.table(near_id)
    near_id <- near_id[,near_top_10 := .N, by=id]

    # Add this new derived variable back into the main dataset
    bnb_data <- near_id[bnb_data,on="id"]
    bnb_data <- bnb_data[is.na(near_top_10), near_top_10:=0]|> unique()

    rm(near_id)
    rm(high_low)
    rm(i)
    rm(all_amenities)
    rm(cols)

    bnb_data <- bnb_data %>%
      dplyr::select(-id)

    final_data <- bnb_data
    ```

Below, you can observe two New Orleans maps, showing the location of each Airbnb in the New Orleans area as well as the ones within 1 km of a top 10 TripAdvisor destination:

::: panel-tabset
## Airbnb Location

![](Listing%20Prices.jpeg){fig-align="center" width="532"}

## Top 10 Location

![](Top%2010%20Spots.jpeg){fig-align="center" width="529"}
:::

With these modifications completed, our "final" dataset contained 4,248 observations and 45 covariates. These cleaning and selection steps ensured that the dataset was both accurate and relevant for building a predictive model for Airbnb pricing in the New Orleans region.

# Section 3 - Exploratory Data Analysis

Once we had a dataset we could work with, our next step was to explore more in depth both binary and non-binary covariates.

For each non-binary covariate, we examined key statistics, including the mean, median, standard deviation, minimum, maximum, and the number of missing values. The table below summarizes these findings:

```{r, message = FALSE}

### Sample size
n <- nrow(final_data)
p <- ncol(final_data)

### Report summary statistics
summary_stats <- function(data)
  {
    results <- list()
    # Loop through each column in the dataset
    for (col in names(data)) 
      {
        # Check if the column is numeric and not binary
        unique_values <- unique(na.omit(data[[col]]))
        if (is.numeric(data[[col]]) && length(unique_values) > 2)
          {
            # Calculate summary statistics
            summary_stats <- c(Mean = round(mean(data[[col]], na.rm = TRUE), 3),
                               Median = round(median(data[[col]], na.rm = TRUE), 3),
                               SD = round(sd(data[[col]], na.rm = TRUE), 3),
                               Min = round(min(data[[col]], na.rm = TRUE), 3),
                               Max = round(max(data[[col]], na.rm = TRUE), 3),
                               NAs = sum(is.na(data[[col]])))
      
            # Store the results in a list
            results[[col]] <- summary_stats
          } 
        else 
          {
            message(paste("Skipping binary or non-numeric column:", col))
          }
      }
  
    # Convert the list to a data frame for easier viewing
    results_df <- as.data.frame(do.call(rbind, results))
    return(results_df)
  }

nonbinary_summary <- summary_stats(final_data)

```

```{r}

# Create a kable table and center the text in the cells
kable(nonbinary_summary, align = "ccc",
      caption = "Summary Statistics for Numeric Variables",
      format = "html", escape = FALSE)
  

```

Additionally, we also looked at their distribution by creating boxplots and density plots:

```{r}

## Look at density plots and boxplots of non-binary variables

create_plots <- function(data) {
  # Loop through each column in the dataset
  for (col in names(data)) {
    # Check if the column is numeric and not binary (contains values other than 0 and 1)
    unique_values <- unique(na.omit(data[[col]]))
    if (is.numeric(data[[col]]) && length(unique_values) > 2) {
      # Create boxplot
      boxplot(data[[col]], main = paste("Boxplot"), xlab = col, col = "lightblue")
      
      # Create density plot
      plot(density(data[[col]], na.rm = TRUE), main = paste("Density Plot"), xlab = col, col = "lightgreen")
    } 
  }
}

```

::: panel-tabset
## 1

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_1 <- final_data %>%
  dplyr::select(near_top_10, host_response_rate, host_acceptance_rate,
         host_total_listings_count)

create_plots(selected_columns_1)
```

## 2

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_2 <- final_data %>%
  dplyr::select(latitude, longitude, accommodates, bathrooms)

create_plots(selected_columns_2)
```

## 3

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_3 <- final_data %>%
  dplyr::select(bedrooms, beds, price, minimum_nights_avg_ntm)

create_plots(selected_columns_3)
```

## 4

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_4 <- final_data %>%
  dplyr::select(availability_30, availability_60, availability_90,
         availability_365)

create_plots(selected_columns_4)
```

## 5

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_5 <- final_data %>%
  dplyr::select(number_of_reviews, number_of_reviews_ltm,
         number_of_reviews_l30d, review_scores_rating)

create_plots(selected_columns_5)
```

## 6

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_6 <- final_data %>%
  dplyr::select(review_scores_accuracy, review_scores_cleanliness,
         review_scores_checkin, review_scores_communication)

create_plots(selected_columns_6)
```

## 7

```{r}

par(mfrow = c(2, 4), mar = c(3, 3, 2, 1),
    mgp = c(1.25, 0.5, 0),
    cex.lab = 0.9, cex.main = 0.8)

selected_columns_7 <- final_data %>%
  dplyr::select(review_scores_location, review_scores_value,
         reviews_per_month, host_years)

create_plots(selected_columns_7)
```
:::

From the summary statistics and the distribution above, we observe that most of our covariates are skewed and multimodal.

Additionally, taking a closer look at our dependent variable price, we decided to log-transform our prices, given their right skewness. This helped make the distribution of values for this variable more normal.

::: panel-tabset
## Before

```{r, message=FALSE}

ggplot(final_data,aes(price)) + geom_histogram() + labs(title = "Distribution of AirBNB Prices - New Orleans")

```

## After

```{r, message=FALSE}

ggplot(final_data,aes(log(price))) + geom_histogram() + labs(title = "Distribution of log(AirBNB Prices) - New Orleans")

```
:::

Regarding the binary variables, we looked at the percentage of 1s and 0s:

```{r}

### Report % of binary variables
report_binary_percentages <- function(data) {
  # Initialize an empty list to store results
  results <- list()
  
  # Loop through each column in the dataset
  for (col in names(data)) {
    # Check if the column is binary (contains only 0s, 1s, or NA values)
    unique_values <- unique(na.omit(data[[col]]))
    if (all(unique_values %in% c(0, 1))) {
      # Calculate the percentages of 0s and 1s, ignoring NAs
      percent_1 <- mean(data[[col]] == 1, na.rm = TRUE) * 100
      percent_0 <- mean(data[[col]] == 0, na.rm = TRUE) * 100
      
      # Store the results in a list
      results[[col]] <- c(`% of 1s` = percent_1, `% of 0s` = percent_0)
    }
  }
  
  # Convert the list to a data frame for easier viewing
  results_df <- as.data.frame(do.call(rbind, results))
  return(results_df)
}

binary_summary <- report_binary_percentages(final_data)

```

```{r}

# Create a kable table and center the text in the cells
kable(binary_summary, align = "ccc",
      caption = "Summary Statistics for Binary Variables",
      format = "html")

```

From the table above, we observe that most hosts provide information about their location, include profile pictures, and have verified identities. However, the majority of rentals lack amenities such as patios, backyards, pools, stainless steel appliances, lakeside views, or indoor fireplaces, among other features.

Our hope is that the fact these enhanced amenities are uncommon (as opposed to something common, such as Wifi) will provide a clear signal for predicting prices for more upscale listings.

# Section 4 - Modeling

Before modeling our data, we took a few final steps to ensure our data was in the format required for regression analysis.

### Finalizing the Data

We set up dummy variables for our categorical measures. We also added an interaction variable (number_of_reviews \* review_score_rating) to our data, as we believe that it is possible that more good reviews may have non-linear impact on prices:

```{r}

set.seed(111)

# Location variables are already modelled in via near_top_10
final_data[,c("neighbourhood_cleansed","latitude", "longitude"):=NULL]

# Get rid of rows with NAs
final_data <- na.omit(final_data)


# Create Dummy Variables
inds <- unique(final_data$host_response_time)
final_data[, (inds) := lapply(inds, function(x) {ifelse(host_response_time == x, 1, 0)})]
final_data[,host_response_time:=NULL] ### Drop original column
final_data[,`a few days or more`:=NULL] ### drop one dummy column, to avoid perfect collinearity

inds <- unique(final_data$room_type)
final_data[, (inds) := lapply(inds, function(x) {ifelse(room_type == x, 1, 0)})]
final_data[,room_type:=NULL] ### Drop original column
final_data[,`Shared room`:=NULL] ### drop one dummy column, to avoid perfect collinearity

final_data[,host_owns_gt_5:=ifelse(host_total_listings_count > 5, 1, 0)]
final_data[,host_owns_2_5:=ifelse(host_total_listings_count < 5 & host_total_listings_count > 1, 1,0 )]
final_data[,host_total_listings_count:=NULL]

# Add interaction variable for # of reviews * review 
# we suspect that the number of high review scores might matter
final_data[,review_score_ct_interaction:=review_scores_rating * number_of_reviews]


### Keep a reference dataset
model_data <- copy(final_data)
```

The dataset was split into a training set (80%) and a testing set (20%) to facilitate model selection and prediction analysis. We then did our log-transformation of price, to improve normality of our dependent variable:

```{r}

### Set up train/test split
tt_split <- createDataPartition(
  y = model_data$price,
  p = .80, ## The percentage of data in the training set
  list = FALSE
)

data_train <- model_data[ tt_split,]
data_test <- model_data[ -tt_split,]


# scale price by log, to make the right-skewed distribution look more normal
data_train[,log_price := log(price)]
data_train[,price:=NULL]

data_test[,log_price := log(price)]
data_test[,price:=NULL]
```

After carefully examining the covariates, we proceeded to the modeling phase.

### Modeling

Now that we had our final, cleaned data set, we proceeded to **modeling the price data.** We initially started with a simple linear model, but also created models using stepwise selection, Lasso, and Ridge regression to evaluate whether any of them could enhance the in-sample fit.

The residuals for each model were extremely similar, so we decided to choose our model based off MSPE (mean squared prediction error) from our hold-out sample of data.

::: panel-tabset
## LM

```{r}

simple_lm <- lm(log_price~.,data=data_train)

par(mfrow = c(2,2))
plot(simple_lm)
```

## Stepwise

```{r}

null_model <- lm(log_price ~ 1, data = data_train[complete.cases(data_train)])
full_model <- lm(log_price ~ ., data = data_train[complete.cases(data_train)])

step_model <- step(null_model, scope = list(lower = null_model, 
                                         upper = full_model),
                       k = log(nrow(data_train)), trace = F)

par(mfrow = c(2,2))
plot(step_model)

```

## Lasso

```{r}

lasso_cv <- cv.glmnet(data_train[, !"log_price", with=FALSE] |> as.matrix(),
                         data_train[,log_price] |> as.matrix(),
                         alpha=1,
                         nfolds=10)
par(mfrow = c(2,2))
#plot(lm_lasso_cv)

lasso_model <- glmnet(data_train[, !"log_price", with=FALSE] |> as.matrix(),
                      data_train[,log_price] |> as.matrix(),
                      alpha=1,
                      lambda = lasso_cv$lambda.min)

lasso_fitted <- predict(lasso_cv,
                        newx = data_train[, !"log_price", with = FALSE] |> as.matrix(), 
                       s = lasso_cv$lambda.min)

# Compute residuals
lasso_residuals <- data_train[, log_price] - lasso_fitted

# Compute standardized residuals
std_residuals_lasso <- lasso_residuals / sd(lasso_residuals)

# 1. Residuals vs Fitted
plot(lasso_fitted, lasso_residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

# 2. Normal Q-Q Plot
qqnorm(lasso_residuals, main = "Normal Q-Q")
qqline(lasso_residuals, col = "red", lty = 2)

# 3. Scale-Location Plot
plot(lasso_fitted, sqrt(abs(std_residuals_lasso)),
     main = "Scale-Location",
     xlab = "Fitted Values",
     ylab = expression(sqrt("|Standardized residuals|")),
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

# 4. Residuals vs Leverage
# Leverage and Cook's distance are not directly available from glmnet.
# Approximate by treating leverage as the influence of fitted values.

# Approximation: Simulate leverage-like values (optional)
lasso_leverages <- hatvalues(lm(data_train[, log_price] ~ as.matrix(data_train[, !"log_price", with = FALSE])))

plot(lasso_leverages, lasso_residuals, 
     main = "Residuals vs Leverage",
     xlab = "Leverage",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

```

## Ridge

```{r}

ridge_cv <- cv.glmnet(data_train[, !"log_price", with=FALSE] |> as.matrix(),
                         data_train[,log_price] |> as.matrix(),
                         alpha=0,
                         nfolds=10)

#coef(lm_ridge_cv, s=lm_ridge_cv$lambda.min)
par(mfrow = c(2,2))
#plot(lm_ridge_cv)

ridge_model <- glmnet(data_train[, !"log_price", with=FALSE] |> as.matrix(),
                      data_train[,log_price] |> as.matrix(),
                      alpha=0,
                      lambda = ridge_cv$lambda.min)

ridge_fitted <- predict(ridge_cv,
                        newx = data_train[, !"log_price", with = FALSE] |> as.matrix(), 
                       s = ridge_cv$lambda.min)

# Compute residuals
ridge_residuals <- data_train[, log_price] - ridge_fitted

# Compute standardized residuals
std_residuals_ridge <- ridge_residuals / sd(ridge_residuals)


# 1. Residuals vs Fitted
plot(ridge_fitted, ridge_residuals,
     main = "Residuals vs Fitted",
     xlab = "Fitted Values",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

# 2. Normal Q-Q Plot
qqnorm(ridge_residuals, main = "Normal Q-Q")
qqline(ridge_residuals, col = "red", lty = 2)

# 3. Scale-Location Plot
plot(ridge_fitted, sqrt(abs(std_residuals_ridge)),
     main = "Scale-Location",
     xlab = "Fitted Values",
     ylab = expression(sqrt("|Standardized residuals|")),
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)

# 4. Residuals vs Leverage
# Leverage and Cook's distance are not directly available from glmnet.
# Approximate by treating leverage as the influence of fitted values.

# Approximation: Simulate leverage-like values (optional)
ridge_leverages <- hatvalues(lm(data_train[, log_price] ~ as.matrix(data_train[, !"log_price", with = FALSE])))

plot(ridge_leverages, ridge_residuals, 
     main = "Residuals vs Leverage",
     xlab = "Leverage",
     ylab = "Residuals",
     pch = 1,
     col = "black")
abline(h = 0, col = "red", lty = 2)
```
:::

Our models also had similar MSE/MSPE of around 0.16, which indicates that our model does an effective job of being able to predict prices with the variables it is given. Additionally, because MSE/MSPE are similar, we see no indications of our model over-fitting the training data:

```{r}

# In-sample average sum squared error (ASE)

ASE_lm <- mean(simple_lm$residuals^2)
MSE_lm <- summary(simple_lm)$sigma^2

ASE_step <- mean(step_model$residuals^2)
MSE_step <- summary(step_model)$sigma^2

lasso_residuals <- data_train[, log_price] - lasso_fitted
ASE_lasso <- mean(lasso_residuals^2)

non_zero_coefs <- sum(coef(lasso_model, s = lasso_cv$lambda.min) != 0)
df_lasso <- nrow(data_train) - non_zero_coefs  
RSS_lasso <- sum(lasso_residuals^2)
MSE_lasso <- RSS_lasso / df_lasso

ridge_residuals <- data_train[, log_price] - ridge_fitted
ASE_ridge <- mean(ridge_residuals^2)

#df_rdige <- nrow(data_train) - non_zero_coefs  
#RSS_lasso <- sum(residuals^2)
MSE_ridge <- 0

results_table <- tibble(
  Model = c("OLS", "Stepwise Regression", "Lasso", "Ridge"),
  ASE = c(ASE_lm, ASE_step, ASE_lasso, ASE_ridge),
  MSE = c(MSE_lm, MSE_step, MSE_lasso, MSE_ridge)
)

# Display the table using kable
kable(results_table, align = "cccc", caption = "Model Comparison: ASE and MSE",
      format = "html")
```

Here is the out-of-sample performance for price prediction:

```{r}

x_test <- data_test %>%
  dplyr::select(-log_price)

y_hat_test_ols <- predict(simple_lm, x_test)
MSPE_lm <- mean((data_test$log_price - y_hat_test_ols)^2)

y_hat_test_step <- predict(step_model, x_test)
MSPE_step <- mean((data_test$log_price - y_hat_test_step)^2)

x_test <- as.matrix(x_test)
y_hat_test_lasso <- predict(lasso_model, x_test,
                            s = lasso_cv$lambda.min)
MSPE_lasso <- mean((data_test$log_price - y_hat_test_lasso)^2)

y_hat_test_ridge <- predict(ridge_model, x_test, s = ridge_cv$lambda.min)
MSPE_ridge <- mean((data_test$log_price - y_hat_test_ridge)^2)

results_table <- tibble(
  Model = c("OLS", "Stepwise Regression", "Lasso", "Ridge"),
  MSPE = c(MSPE_lm, MSPE_step, MSPE_lasso, MSPE_ridge)
)

# Display the table using kable
kable(results_table, align = "cccc", caption = "Out of Sample Performance",
      format = "html")
```

## The Winner: Lasso

We chose the Lasso model to be our final price prediction model because it had 1) the lowest MSPE and 2) was more parsimonious than models of comparable accuracy.

This model fit the data using 36 of 48 variables supplied to it, and achieved an MSPE of 0.1586327.

Below are the fitted model coefficients for each:

(**NOTE:** coefficients for Lasso and Ridge regression were found via 10-fold cross-validation)

::: panel-tabset
## 1) Lasso

```{r}
lasso_model
coef(lasso_model)
```

## 2) Ridge

```{r}
ridge_model
coef(ridge_model)
```

## 3) LM

```{r}

summary(simple_lm)
```

## 4) Stepwise

```{r}

summary(step_model)

```
:::

# Section 5 - Discussion

Our pricing model did a solid job of predicting the price of Airbnb listings in New Orleans, with \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

The model revealed that the most salient factors that determine an Airbnb listing's price in this market were:

-   TBD

-   TBD

-   TBD

-   TBD

## Further Study

**Generalizing for other markets**: This model could easily be applied to Airbnb rentals from other markets, to assess how well it describes trends in those markets. By re-training across multiple markets, this model could be generalized to reliably predict prices of ANY Airbnb listing.

**Data Supplementation**: Our model could also benefit from adding publicly-available data to improve predictions. There were many other potential variables related to location that likely impact prices - CRIME\*, transit, local housing prices.....Many of these variables have a publicly-available data source (such as the police department) to find this data. If we had more time, we would have liked to follow up on at least some of these factors to assess their impact on pricing.
